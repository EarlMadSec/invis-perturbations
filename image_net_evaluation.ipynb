{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import image_net_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(image_net_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "pretrained_model = models.resnet101(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "\n",
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMask(new_w, batch_limits, mask, c_limits, sig_height, conv_size, precision_depth=2):\n",
    "    \n",
    "    lay = torch.nn.Conv1d(1,1,conv_size)\n",
    "\n",
    "    #Manually setting the weights and bias so the  shutter acts as a box filter\n",
    "    lay.weight.data = torch.full([1,1,conv_size,1], 1/conv_size, requires_grad=True, dtype=torch.float, device=device)\n",
    "    lay.bias.data = torch.zeros(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "    \n",
    "    sz = new_w.shape[1]\n",
    "    batch = batch_limits[1]-batch_limits[0]          \n",
    "    \n",
    "    # EOT sampling for ambient light and shift\n",
    "    c = torch.rand([batch,1,1,1], device=device) * (c_limits[1] - c_limits[0]) + c_limits[0]\n",
    "    shift = torch.tensor(range(batch_limits[0],batch_limits[1]), dtype=torch.int)\n",
    "    #shift = torch.from_numpy(np.array(range(0,batch,1)))\n",
    "    \n",
    "    #Shift the signal\n",
    "    new_w = shift_operation(new_w.unsqueeze(0).repeat(batch,1,1,1).view(-1, sz, 1), shift, device).view(batch,3,sz,1)\n",
    "    \n",
    "    #Fit w into the range [0,1]. new_w is the same as ft\n",
    "    #new_w = .5 * (torch.tanh(ootn) + 1)\n",
    "    \n",
    "    #stack the signal to fit the input size\n",
    "    new_w = torch.stack([stack(ooti,sig_height,device) for ooti in new_w])\n",
    "    \n",
    "    #precision limit\n",
    "    new_w = diff_round(new_w, precision_depth)\n",
    "    \n",
    "    #Convolution of ft and the shutter\n",
    "    #gy = lay(new_w.unsqueeze(0).view([3,1,228,batch])).view([batch,3,224,1])\n",
    "    gy = lay(new_w.transpose(0,3).transpose(0,1)).transpose(0,1).transpose(0,3)\n",
    "\n",
    "    #Mask the signal to only affect the object\n",
    "    if mask:\n",
    "        gy_mask = torch.mul(gy,torch.transpose(mask,1,0))\n",
    "    else:\n",
    "        gy_mask = gy\n",
    "\n",
    "    return (c + (1-c)*gy_mask), new_w, c\n",
    "\n",
    "def upsample2d(img, size=224):\n",
    "    upsample = nn.Upsample(size=size, mode='bilinear', align_corners=False)\n",
    "    return upsample(torch.unsqueeze(img, 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_dir = \"input_dump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_path = []\n",
    "for subdir, dirs, files in os.walk(signal_dir):\n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "        \n",
    "        if filepath.endswith(\".json\"):\n",
    "            signal_path.append(filepath)\n",
    "signal_configs = []\n",
    "for sig in signal_path:\n",
    "    with open(sig) as f:\n",
    "        dd = json.load(f)\n",
    "        dd['file_path'] = sig\n",
    "        signal_configs.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_df = pd.DataFrame(signal_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_df[signal_df['classidx']==\"456\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = signal_df[signal_df['classidx']==\"456\"].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating a signal\n",
    "def load_signal(signal_path):\n",
    "    return torch.load(signal_path.split(\".\")[0]+'.pt', map_location=device)[0]\n",
    "\n",
    "def apply_signal(signal, sig_height, conv_size , batch_size, offset, original_images, apply_trans, ambient_light, targidx, classidx):\n",
    "    target_acc, class_acc, max_acc = [], [], []\n",
    "    gy, www, c = applyMask(signal, [offset, offset+batch_size], None, [0,0], sig_height, conv_size, 0)\n",
    "    \n",
    "    img_t2, img_b2, img_f2 = image_net_python.get_image_trip(original_images[0], original_images[1], device, apply_trans, ambient_light)\n",
    "    inp2 = torch.pow(0.0000001 + torch.pow(img_b2,2.2) + gy*(torch.pow(img_f2,2.2)-torch.pow(img_t2,2.2)), 1/2.2)\n",
    "    inp2 = torch.cat([forward_normalize(upsample2d(i,224)).unsqueeze(0) for i in inp2])\n",
    "    inp2 = inp2.to(device)\n",
    "    out2 = pretrained_model(inp2)\n",
    "    prob2 = lay2(out2)\n",
    "    maxcls2 = prob2.max(1)\n",
    "    for i in range(batch_size):\n",
    "        target_acc.append(prob2[i][targidx].item())\n",
    "        class_acc.append(prob2[i][classidx].item())\n",
    "        max_acc.append((maxcls2.indices[i].item(), prob2[i][maxcls2.indices[i].item()].item()))\n",
    "    del img_t2\n",
    "    del img_f2\n",
    "    del inp2\n",
    "    del out2\n",
    "    del prob2\n",
    "    del maxcls2\n",
    "    torch.cuda.empty_cache()\n",
    "    return target_acc, class_acc, max_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "def run_evaluation(config_dict):\n",
    "    sig = load_signal(config_dict['file_path'])\n",
    "    sig_height, conv_size = sig.size()[1], sig.size()[1]-251\n",
    "    target_acc, class_acc, max_acc = [], [], []\n",
    "    for i in range(0, int(sig_height/batch_size)*batch_size,batch_size):\n",
    "        a,b,c = apply_signal(sig, sig_height, conv_size , batch_size, i, original_images, config_dict['apply_transformations']==\"True\", int(config_dict['ambient_light']), int(config_dict['targidx']), int(config_dict['classidx']))\n",
    "        target_acc += a\n",
    "        class_acc += b\n",
    "        max_acc += c\n",
    "    \n",
    "    return target_acc, class_acc, max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = [Image.open('images/original/col_amb_up.jpg'), Image.open('images/original/col_full_up.jpg')]\n",
    "img_path = r'images/imagenet_classes/imagenet_images/imagenet_images/bow/384287867_37a8913d93_b.jpg'\n",
    "img = Image.open(img_path)\n",
    "img_full, img = TF.adjust_brightness(img, float(160/np.average(img))), TF.adjust_brightness(img, float(80/np.average(img)))\n",
    "original_images = [img, img_full]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(original_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_acc, class_acc, max_acc = run_evaluation(config[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_class_acc = np.array(class_acc).mean()\n",
    "avg_target_acc = np.array(target_acc).mean()\n",
    "dodge_success = np.array([1 if x[0] not in list(config[0]['classes_to_skip']) else 0 for x in max_acc]).mean()\n",
    "target_success = np.array([1 if x[0]==int(config[0]['targidx']) else 0 for x in max_acc]).mean()\n",
    "output_classes = {}\n",
    "for cl in max_acc: \n",
    "    if cl[0] not in output_classes: output_classes[cl[0]]=0\n",
    "    output_classes[cl[0]]+=1\n",
    "print(\"Avg. accuracy for {} (original) - {}\".format(class_dict[int(config[0]['classidx'])],avg_class_acc))\n",
    "print(\"Avg. accuracy for {} (target)- {}\".format(class_dict[int(config[0]['targidx'])],avg_target_acc))\n",
    "print(\"Dodge success outside classes to skip - {}\".format(dodge_success))\n",
    "print(\"Target success - {}\".format(target_success))\n",
    "print(\"Output class histogram - \",output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_values = []\n",
    "for conf in config:\n",
    "    target_acc, class_acc, max_acc = run_evaluation(conf)\n",
    "    \n",
    "    avg_class_acc = np.array(class_acc).mean()\n",
    "    avg_target_acc = np.array(target_acc).mean()\n",
    "    dodge_success = np.array([1 if x[0] not in list(conf['classes_to_skip']) else 0 for x in max_acc])\n",
    "    target_success = np.array([1 if x[0]==int(conf['targidx']) else 0 for x in max_acc])\n",
    "    output_classes = {}\n",
    "    for cl in max_acc: \n",
    "        if cl[0] not in output_classes: output_classes[cl[0]]=0\n",
    "        output_classes[cl[0]]+=1\n",
    "    print(\"Avg. accuracy for {} (original) - {}\".format(class_dict[int(conf['classidx'])],avg_class_acc))\n",
    "    print(\"Avg. accuracy for {} (target)- {}\".format(class_dict[int(conf['targidx'])],avg_target_acc))\n",
    "    print(\"Dodge success outside classes to skip - {}\".format(dodge_success.mean()))\n",
    "    print(\"Target success - {}\".format(target_success.mean()))\n",
    "    print(\"Output class histogram - \",output_classes)\n",
    "    output_values.append((class_acc,target_acc,target_success,output_classes))\n",
    "\n",
    "#output_values_config = [\"Class acc\", \"Target acc\", \"Dodge(similar)\",\"Attack Success\"]\n",
    "output_values_config = [\"Class acc\", \"Target acc\",\"Attack Success\"]\n",
    "input_configs = []\n",
    "for t in config:\n",
    "    input_configs.append(t['targidx']+\"-\"+str(class_dict[int(t['targidx'])][:10]))\n",
    "    #input_configs.append(str(t)+\"-adv\")\n",
    "\n",
    "df_list = []\n",
    "for i,inpc in enumerate(input_configs):\n",
    "    config_name = inpc\n",
    "    for j in range(3):\n",
    "        for k in range(len(output_values[i][j])):\n",
    "            df_list.append([config_name, output_values[i][j][k], output_values_config[j]])\n",
    "df_results = pd.DataFrame(df_list, columns = ['config','metric_value','metric_type'])\n",
    "\n",
    "plot_title = 'Selected Classes; 10k; bow'\n",
    "sns.barplot(data=df_results, x='config', y='metric_value',hue=\"metric_type\").set_title(plot_title)\n",
    "plt.xticks(rotation=30)\n",
    "#plt.savefig('plots/'+plot_title+'.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
