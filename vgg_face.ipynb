{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "from utils import *\n",
    "from resnet50_ft_dims_2048 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display image and predicted class\n",
    "def imstats(name):\n",
    "    # read in image and view it\n",
    "    x = readim(name, forward_normalize) \n",
    "    imshow_tensor(x[0], inv_normalize)\n",
    "\n",
    "    #get predicted class and probability\n",
    "    prob = lay2(pretrained_model(x.cuda()))\n",
    "    maxcls = prob.max(1)\n",
    "    print(\"Class is {} ({}) with confidence {}%\".format(maxcls.indices.item(),class_dict[maxcls.indices.item()],round(maxcls.values.item()*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[91.4953, 103.8827, 131.0912],\n",
    "                                                              std=[1, 1, 1])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet50_ft(\"vgg_face_testimages/resnet50_ft_dims_2048.pth\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img, shape=None):\n",
    "    short_size = 224.0\n",
    "    crop_size = shape\n",
    "    im_shape = np.array(img.size)    # in the format of (width, height, *)\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    ratio = float(short_size) / np.min(im_shape)\n",
    "    img = img.resize(size=(int(np.ceil(im_shape[0] * ratio)),   # width\n",
    "                           int(np.ceil(im_shape[1] * ratio))),  # height\n",
    "                     resample=PIL.Image.BILINEAR)\n",
    "\n",
    "    x = np.array(img)  # image has been transposed into (height, width)\n",
    "    newshape = x.shape[:2]\n",
    "    h_start = (newshape[0] - crop_size[0])//2\n",
    "    w_start = (newshape[1] - crop_size[1])//2\n",
    "    x = x[h_start:h_start+crop_size[0], w_start:w_start+crop_size[1]]\n",
    "    x = x - mean\n",
    "    return x\n",
    "\n",
    "def image_encoding(model, images):\n",
    "    print('==> compute image-level feature encoding.')\n",
    "    num_faces = len(images)\n",
    "\n",
    "    im_array = np.array([load_data(img=i, shape=(224, 224, 3)) for i in images])\n",
    "    im_tensor = torch.Tensor(im_array.transpose(0, 3, 1, 2))\n",
    "    im_tensor = im_tensor.to(device)\n",
    "    f  = model(im_tensor)\n",
    "    classif = f[0]\n",
    "    feat = f[1].detach().cpu().numpy()[:, :, 0, 0]\n",
    "    face_feats = feat / np.sqrt(np.sum(feat ** 2, -1, keepdims=True))\n",
    "    return classif, face_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (131.0912, 103.8827, 91.4953)\n",
    "batch_size = 1\n",
    "\n",
    "#image_dir = \"vgg_face_testimages/vgg_face2/samples (test set)/tight_crop (used for training)/n000106/\"\n",
    "#input_im = Image.open(image_dir+\"0002_01.jpg\")\n",
    "\n",
    "IMG_URL = \"https://images.ctfassets.net/p0qf7j048i0q/34947631635447C28C42A07B6D9CE189/74fa2cbbc95f35863902ff37a2238fab/AP578484237061.jpg?w=1000&h=750&fit=fill&fm=webp\"\n",
    "response = requests.get(IMG_URL)\n",
    "input_im = Image.open(io.BytesIO(response.content))\n",
    "\n",
    "display(input_im)\n",
    "\n",
    "out, face_feats = image_encoding(model, [input_im])\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out\n",
    "print(pred.size())\n",
    "\n",
    "#Use softmax to get predicted probability and view it\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(pred)\n",
    "print(prob)\n",
    "maxOcls = prob.max(1)\n",
    "print(maxOcls)\n",
    "print(prob[0][1509])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('renv': venv)",
   "language": "python",
   "name": "python36964bitrenvvenv63ce17223ffb457ca2f50b70abb95e80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
