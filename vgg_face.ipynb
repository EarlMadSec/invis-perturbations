{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "from utils import *\n",
    "from resnet50_ft_dims_2048 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display image and predicted class\n",
    "def imstats(name):\n",
    "    # read in image and view it\n",
    "    x = readim(name, forward_normalize) \n",
    "    imshow_tensor(x[0], inv_normalize)\n",
    "\n",
    "    #get predicted class and probability\n",
    "    prob = lay2(pretrained_model(x.cuda()))\n",
    "    maxcls = prob.max(1)\n",
    "    print(\"Class is {} ({}) with confidence {}%\".format(maxcls.indices.item(),class_dict[maxcls.indices.item()],round(maxcls.values.item()*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[91.4953, 103.8827, 131.0912],\n",
    "                                                              std=[1, 1, 1])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet50_ft(\"vgg_face_testimages/resnet50_ft_dims_2048.pth\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img, shape=None):\n",
    "    short_size = 224.0\n",
    "    crop_size = shape\n",
    "    im_shape = np.array(img.size)    # in the format of (width, height, *)\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    ratio = float(short_size) / np.min(im_shape)\n",
    "    img = img.resize(size=(int(np.ceil(im_shape[0] * ratio)),   # width\n",
    "                           int(np.ceil(im_shape[1] * ratio))),  # height\n",
    "                     resample=PIL.Image.BILINEAR)\n",
    "\n",
    "    x = np.array(img)  # image has been transposed into (height, width)\n",
    "    newshape = x.shape[:2]\n",
    "    h_start = (newshape[0] - crop_size[0])//2\n",
    "    w_start = (newshape[1] - crop_size[1])//2\n",
    "    x = x[h_start:h_start+crop_size[0], w_start:w_start+crop_size[1]]\n",
    "    x = x - mean\n",
    "    return x\n",
    "\n",
    "def image_encoding(model, images):\n",
    "    #print('==> compute image-level feature encoding.')\n",
    "    num_faces = len(images)\n",
    "    im_array = np.array([load_data(img=i, shape=(224, 224, 3)) for i in images])\n",
    "    im_tensor = torch.Tensor(im_array.transpose(0, 3, 1, 2))\n",
    "    im_tensor = im_tensor.to(device)\n",
    "    f  = model(im_tensor)\n",
    "    classif = f[0]\n",
    "    feat = f[1].detach().cpu().numpy()[:, :, 0, 0]\n",
    "    face_feats = feat / np.sqrt(np.sum(feat ** 2, -1, keepdims=True))\n",
    "    return classif, face_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_features(images):\n",
    "    out, face_feats = image_encoding(model, images)\n",
    "    return out, face_feats\n",
    "\n",
    "def fetch_images(paths):\n",
    "    images = []\n",
    "    for im in paths:\n",
    "        images.append(Image.open(im))\n",
    "    return images\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    svm_input = []\n",
    "    svm_labels = []\n",
    "    for i in tqdm(range(0,len(dataset),30)):\n",
    "        batch = dataset[i:min(i+30,len(dataset))]\n",
    "        images = fetch_images(batch)\n",
    "        out, feats = predict_features(images)\n",
    "        for ind, path in enumerate(batch):\n",
    "            svm_input.append(feats[ind])\n",
    "            svm_labels.append(int(path.split(\"/\")[-2].split(\"n00\")[-1]))\n",
    "        del images\n",
    "        del feats\n",
    "        del out\n",
    "    return svm_input, svm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_occuring_label(pred):\n",
    "    #Use softmax to get predicted probability and view it\n",
    "    lay2 = torch.nn.Softmax(dim=1)\n",
    "    prob = lay2(pred)\n",
    "    maxOcls = prob.max(1)\n",
    "    labels, counts = maxOcls.indices.unique(return_counts=True)\n",
    "    return labels[counts.max(0)[1]], counts[counts.max(0)[1]], counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (131.0912, 103.8827, 91.4953)\n",
    "batch_size = 1\n",
    "\n",
    "vgg_test_dir = \"/nobackup/vgg2face/test/\"\n",
    "class_dir_list = os.listdir(vgg_test_dir)\n",
    "train_dataset_paths = []\n",
    "test_dataset_paths = []\n",
    "\n",
    "for class_dir in class_dir_list:\n",
    "    class_dir = vgg_test_dir+class_dir+\"/\"\n",
    "    all_images = []\n",
    "    for image in os.listdir(class_dir):\n",
    "        all_images.append(class_dir+image)\n",
    "    random.shuffle(all_images)\n",
    "    test_dataset_paths += all_images[:50]\n",
    "    train_dataset_paths += all_images[50:]\n",
    "\n",
    "train_input, train_labels = process_dataset(train_dataset_paths)\n",
    "test_input, test_labels = process_dataset(test_dataset_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "X_train = np.array(train_input)\n",
    "y_train = np.array(train_labels)\n",
    "class_labels = np.unique(y_train)\n",
    "class_dict = {}\n",
    "for i,v in enumerate(class_labels):\n",
    "    class_dict[v]=i\n",
    "y_train = np.array([class_dict[x] for x in train_labels])\n",
    "X_val = np.array(test_input)\n",
    "y_val = np.array([class_dict[x] for x in test_labels])\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0007\n",
    "NUM_FEATURES = 2048\n",
    "NUM_CLASSES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_feature, 1024)\n",
    "        self.layer_2 = nn.Linear(1024, 512)\n",
    "        self.layer_3 = nn.Linear(512, 128)\n",
    "        self.layer_out = nn.Linear(128, num_class) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(1024)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "model_nn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_nn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc) * 100\n",
    "    \n",
    "    return acc\n",
    "\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    model_nn.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model_nn(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        model_nn.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model_nn(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_nn.state_dict(), \"vgg2_test_classifier.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('renv': venv)",
   "language": "python",
   "name": "python36964bitrenvvenv63ce17223ffb457ca2f50b70abb95e80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
