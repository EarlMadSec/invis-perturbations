{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display image and predicted class\n",
    "def imstats(name):\n",
    "    # read in image and view it\n",
    "    x = readim(name, forward_normalize) \n",
    "    imshow_tensor(x[0], inv_normalize)\n",
    "\n",
    "    #get predicted class and probability\n",
    "    prob = lay2(pretrained_model(x.cuda()))\n",
    "    maxcls = prob.max(1)\n",
    "    print(\"Class is {} ({}) with confidence {}%\".format(maxcls.indices.item(),class_dict[maxcls.indices.item()],round(maxcls.values.item()*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classes\n",
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "#Retrieve image and assign class\n",
    "#IMG_URL = \"https://bloximages.chicago2.vip.townnews.com/chippewa.com/content/tncms/assets/v3/editorial/c/ca/cca0d87d-8d7c-520f-b012-bc065d18b3dd/553cbf5aa0ffb.image.jpg?crop=200%2C200%2C0%2C8&resize=1200%2C1200&order=crop%2Cresize\"\n",
    "#IMG_URL = \"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSa3p29cIlgj0yQwMuLZzh8PKkgBUzbPLWrU-7K79DjDL498JsA&usqp=CAU\"\n",
    "IMG_URL = \"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQ5X7yUzT_3kR7guoqCF5zEFZkesvEkagag8Y1KFJoULsXrL3h0&usqp=CAU\"\n",
    "classidx = 504\n",
    "response = requests.get(IMG_URL)\n",
    "img = Image.open(io.BytesIO(response.content))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "forward_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(type(img))\n",
    "img_input = model_transform(img)\n",
    "#img_input = img_input.unsqueeze(0)\n",
    "#img_input = Variable(img_input)\n",
    "img_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = models.resnet101(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "#Pass input through model\n",
    "img_input = img_input.to(device)\n",
    "pred = pretrained_model(img_input.unsqueeze(0))\n",
    "\n",
    "#Use softmax to get predicted probability and view it\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(pred)\n",
    "maxOcls = prob.max(1)\n",
    "print(\"Class is {} ({}) with confidence {}%\".format(maxOcls.indices.item(),class_dict[maxOcls.indices.item()],maxOcls.values.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_mask(input_image):\n",
    "    #print(test[-3][:])\n",
    "    test = inv_normalize(input_image)\n",
    "    test = np.uint8(test.numpy()*255).transpose((1,2,0))\n",
    "    test = 1 - test\n",
    "    #test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(test, cv2.COLOR_RGB2GRAY)\n",
    "    th, thgray = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY_INV); \n",
    "    #blurred = cv2.GaussianBlur(thgray, (9, 9), 0)\n",
    "    th, blurred = cv2.threshold(thgray, 50, 255, cv2.THRESH_BINARY)\n",
    "    edged = cv2.Canny(blurred, 1, 250, L2gradient=True)   \n",
    "    #imgplot = plt.imshow(blurred, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    return blurred\n",
    "    #imshow(input_image[0])\n",
    "    #applying closing function\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    closed = cv2.morphologyEx(blurred, cv2.MORPH_CLOSE, kernel)    #finding_contours\n",
    "    (cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)   \n",
    "    #cnts = [] #only for plate class \n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        cv2.drawContours(closed, [approx], -1, (0, 255, 0), 2)    \n",
    "    th, im_th = cv2.threshold(closed, 254, 255, cv2.THRESH_BINARY_INV);    \n",
    "    h, w = im_th.shape[:2]\n",
    "    im_floodfill = im_th.copy()\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 0);    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)    # Combine the two images to get the foreground.\n",
    "    #imshow(im_th)\n",
    "    #plt.show()\n",
    "    #imshow(im_floodfill_inv)\n",
    "    #plt.show()\n",
    "    im_out = im_th ^ im_floodfill_inv\n",
    "    im_out = cv2.GaussianBlur(im_out, (3, 3), 0)\n",
    "    return im_out\n",
    "print(type(img_input))\n",
    "imshow_tensor(img_input.cpu(),inv_normalize)\n",
    "imgplot = plt.imshow(get_object_mask(img_input.cpu()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing classes. Likely unnecessary\n",
    "adv = [598, 697, 898, 567, 69, 855, 641, 600, 942]\n",
    "fold = \"c0.3_sz228_shiftF/\"\n",
    "#targidx = np.random.randint(0,1000)\n",
    "targidx = adv[1]\n",
    "print(targidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sz = 200   #Length of input signal\n",
    "c = .5     #Ambient light ratio\n",
    "c_limits = [0,0]\n",
    "batch = 64\n",
    "channels = 1\n",
    "# change of variable term to optimise on\n",
    "w = torch.rand([channels,sz,1], requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "#Create the mask to only illuminate the object\n",
    "mask = torch.tensor(get_object_mask(img_input.cpu()), dtype=torch.float, device=device)\n",
    "mask = mask / torch.max(mask)\n",
    "\n",
    "\n",
    "#Target and original class labels\n",
    "target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "orig = torch.tensor([classidx], dtype=torch.long, device=device)\n",
    "\n",
    "#Model parameters\n",
    "lr = 1e-1\n",
    "n_epochs = 5\n",
    "optimizer = optim.SGD([w], lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "classes_to_skip = [classidx]#, 901, 968, 647,899]#,725,441,505,441,969,438,967]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track the loss to target and original class\n",
    "targloss = []\n",
    "origloss = []\n",
    "\n",
    "#obj_dict = {}\n",
    "\n",
    "#Optimisation loop. initially untargeted\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    \n",
    "    #Switch to targeted at halfway point\n",
    "    half = epoch < n_epochs//6\n",
    "    if epoch == n_epochs//6:\n",
    "        tops = out.topk(20).indices[0]\n",
    "        for t in tops:\n",
    "            #print(t.item())\n",
    "            if t.item() not in classes_to_skip: \n",
    "                targidx = t.item()\n",
    "        #targidx = tops[0].item() if tops[0].item() != classidx else tops[1].item()\n",
    "        target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "        print(\"Switching from untarget to target {}\".format(targidx))\n",
    "    \n",
    "    if channels==1:\n",
    "        n_w = w.repeat(3,1,1)\n",
    "    else:\n",
    "        n_w = w\n",
    "    gy, new_w = fttogy(n_w, batch, mask, c_limits, 263)\n",
    "    \n",
    "    inp = gy*inv_normalize(img_input)   \n",
    "    inp = torch.cat([forward_normalize(i).unsqueeze(0) for i in inp])\n",
    "    out = pretrained_model(inp)\n",
    "\n",
    "    #Calculate Loss depended on if targeted or untargeted\n",
    "    if not half: targLoss = loss_fn(out, target.repeat(batch))\n",
    "    origLoss = loss_fn(out, orig.repeat(batch))\n",
    "    loss = -origLoss if half else targLoss\n",
    "    if epoch%100 == 0:\n",
    "        targloss.append(0 if half else targLoss)\n",
    "        origloss.append(origLoss)\n",
    "        if not half: print(targLoss, origLoss) \n",
    "    loss.backward()   \n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    del loss\n",
    "    if epoch!=n_epochs-1:\n",
    "        del inp\n",
    "        del new_w\n",
    "    #else:\n",
    "        #saving w to be used for prediction\n",
    "        #torch.save(n_w,'w_0.5_764.pt')\n",
    "\n",
    "    #Code to check gpu allocation    \n",
    "    '''\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                #print(type(obj), obj.size())\n",
    "                if type(obj) not in obj_dict:\n",
    "                    obj_dict[type(obj)] = 1\n",
    "                else:\n",
    "                    obj_dict[type(obj)] += 1\n",
    "        except: pass\n",
    "    print(obj_dict)\n",
    "    obj_dict.clear()\n",
    "    '''\n",
    "    torch.cuda.empty_cache()\n",
    "#View original loss and target loss\n",
    "plt.plot(targloss, label=\"target\")\n",
    "plt.plot(origloss, label=\"original\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View original loss and target loss\n",
    "plt.plot(targloss, label=\"target\")\n",
    "plt.plot(origloss, label=\"original\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create softmax layer to view probabilities\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(out)\n",
    "maxcls = prob.max(1)\n",
    "#print(maxcls)\n",
    "for i in range(batch):\n",
    "    #print(\"alpha: {}\".format(c[i].item()))\n",
    "    print(\"target {}: {}%\".format(targidx,prob[i][targidx]))\n",
    "    print(\"Orig {}: {}%\".format(classidx,prob[i][classidx]))\n",
    "    print(\"Class is {} ({}) with confidence {}%\\n\".format(maxcls.indices[i].item(),class_dict[maxcls.indices[i].item()],maxcls.values[i].item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View input image and adversarial image. Save both\n",
    "imshow_tensor(forward_normalize(inv_normalize(img_input.cpu())*c), inv_normalize)\n",
    "imshow_tensor(inp[2].detach().cpu(),inv_normalize)\n",
    "#saveim(img_input.cpu(), \"original_{}.png\".format(classidx),inv_normalize)\n",
    "#saveim(forward_normalize(inv_normalize(img_input.cpu())*c), \"original_c{}_{}.png\".format(c,classidx),inv_normalize)\n",
    "#saveim(inp, \"src{}_c{}_sz{}_tg{}.png\".format(classidx,c,sz,targidx),inv_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.flatten(new_w[0][0]).detach().cpu(), label=\"w_r\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(torch.flatten(new_w[0][1]).detach().cpu(), label=\"w_g\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(torch.flatten(new_w[0][2]).detach().cpu(), label=\"w_b\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the attack signal colour channels separately\n",
    "test = torch.ones([3,224,224], device=device)\n",
    "re = test * split(gy_mask,0)\n",
    "gr = test * split(gy_mask,1)\n",
    "bl = test * split(gy_mask,2)\n",
    "imshow_tensor(forward_normalize(re.detach().cpu()), inv_normalize)\n",
    "imshow_tensor(forward_normalize(gr.detach().cpu()), inv_normalize)\n",
    "imshow_tensor(forward_normalize(bl.detach().cpu()), inv_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imstats(\"original_919.png\")\n",
    "imstats(\"original_c0.5_919.png\")\n",
    "imstats(\"original_c0.3_919.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classes\n",
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "#Retrieve image and assign class\n",
    "IMG_URL = \"https://bloximages.chicago2.vip.townnews.com/chippewa.com/content/tncms/assets/v3/editorial/c/ca/cca0d87d-8d7c-520f-b012-bc065d18b3dd/553cbf5aa0ffb.image.jpg?crop=200%2C200%2C0%2C8&resize=1200%2C1200&order=crop%2Cresize\"\n",
    "classidx = 919\n",
    "response = requests.get(IMG_URL)\n",
    "img = Image.open(io.BytesIO(response.content))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = 5\n",
    "test = torch.tensor([[0., 1., 1., 1., 1.],\n",
    "        [2., 2., 2., 2., 2.],\n",
    "        [3., 3., 3., 3., 3.]],device=device)\n",
    "print(test)\n",
    "test_batch = 2\n",
    "test_shifted = shift_operation(test.unsqueeze(0).repeat(test_batch,1,1,1).view(-1, test_length, 1), [0,1]).view(test_batch,3,test_length,1)\n",
    "print(test_shifted.shape)\n",
    "print(test_shifted)\n",
    "test_reshaped = test_shifted.view([3,1,test_length,test_batch])\n",
    "treshape = test_shifted.transpose(0,3).transpose(0,1)\n",
    "print(test_reshaped.shape)\n",
    "print(test_reshaped)\n",
    "print(treshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
