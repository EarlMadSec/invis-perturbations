{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def imshow_tensor(img):\n",
    "    img = inv_normalize(img)     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "def saveim(img, name):\n",
    "    save_image(img, name)\n",
    "def readim(name):\n",
    "    image = Image.open(name)\n",
    "    x = TF.to_tensor(image)\n",
    "    tform = transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))\n",
    "    x = tform(x)\n",
    "    x.unsqueeze_(0)\n",
    "    return x\n",
    "def stack(w,size):\n",
    "    dim = len(torch.flatten(w))\n",
    "    if dim == size:\n",
    "        return w\n",
    "    ide = torch.eye(dim, requires_grad=True, dtype=torch.float, device=device)\n",
    "    zer = torch.zeros([1,dim], requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "    n = size // dim\n",
    "    m = size % dim\n",
    "    nsum = torch.zeros([1,1,size,1], requires_grad=True, dtype=torch.float, device=device)\n",
    "    ides = torch.cat(n*[ide])\n",
    "    if m != 0:\n",
    "        zers = torch.cat(m*[zer])\n",
    "        mat = torch.cat([ides,zers])\n",
    "    else:\n",
    "        mat = ides\n",
    "    nsum = nsum + torch.matmul(mat,w)\n",
    "    if m == 0:\n",
    "        return nsum\n",
    "    t = []\n",
    "    for i in range(m):\n",
    "        t.append(torch.tensor([1 if x == i else 0 for x in range(dim)],requires_grad=True, dtype=torch.float), device=device)\n",
    "\n",
    "    mat2 = torch.cat([torch.cat(dim*n*[zer]),torch.stack(t)])\n",
    "    nsum = nsum + torch.matmul(mat2,w)\n",
    "    return nsum\n",
    "def gamma_correction(img, factor):\n",
    "    return ((img+0.5)**factor)-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "IMG_URL = \"https://farm1.static.flickr.com/21/34945218_3cea57417c.jpg\"\n",
    "classidx = 397\n",
    "response = requests.get(IMG_URL)\n",
    "img = Image.open(io.BytesIO(response.content))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "\n",
    "\n",
    "print(type(img))\n",
    "img_input = model_transform(img)\n",
    "img_input = img_input.unsqueeze(0)\n",
    "img_input = Variable(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = models.resnet101(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "prediction = pretrained_model(img_input)\n",
    "prediction_label = prediction.data.numpy().argmax()\n",
    "print(prediction_label)\n",
    "print(class_dict[prediction_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_mask(input_image):\n",
    "    #print(test[-3][:])\n",
    "    test = inv_normalize(input_image)\n",
    "    test = np.uint8(test.numpy()*255).transpose((1,2,0))\n",
    "    #test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(test, cv2.COLOR_RGB2GRAY)\n",
    "    th, thgray = cv2.threshold(gray, 30, 255, cv2.THRESH_BINARY_INV); \n",
    "    #blurred = cv2.GaussianBlur(thgray, (9, 9), 0)\n",
    "    th, blurred = cv2.threshold(thgray, 50, 255, cv2.THRESH_BINARY_INV); \n",
    "    edged = cv2.Canny(blurred, 1, 250, L2gradient=True)   \n",
    "    #imgplot = plt.imshow(blurred, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    #imshow(input_image[0])\n",
    "    #applying closing function\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(9,9))\n",
    "    closed = cv2.morphologyEx(blurred, cv2.MORPH_CLOSE, kernel)    #finding_contours\n",
    "    (cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)    \n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        cv2.drawContours(closed, [approx], -1, (0, 255, 0), 2)    \n",
    "    th, im_th = cv2.threshold(closed, 254, 255, cv2.THRESH_BINARY_INV);    \n",
    "    h, w = im_th.shape[:2]\n",
    "    im_floodfill = im_th.copy()\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (200,0), 0);    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)    # Combine the two images to get the foreground.\n",
    "    im_out = im_th ^ im_floodfill_inv\n",
    "    im_out = cv2.GaussianBlur(im_out, (3, 3), 0)\n",
    "    return im_out\n",
    "print(type(img_input[0]))\n",
    "imshow_tensor(img_input[0])\n",
    "imgplot = plt.imshow(get_object_mask(img_input[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input time signal\n",
    "sz = 228\n",
    "c = 0.3\n",
    "w = torch.rand([1,1,sz,1], requires_grad=True, dtype=torch.float, device=device)\n",
    "targidx = 200\n",
    "#Does not work since no useful gradients\n",
    "#Aw = torch.rand([1], requires_grad=True, dtype=torch.float, device=device) #amplitude and frequency\n",
    "#sample = torch.linspace(0, 4, sz, dtype=torch.float, device=device)  #Sampling rate, do the math to get this\n",
    "#w = torch.sin(100*Aw[0]*sample).view([1,1,sz,1]) #Sample the sine wave\n",
    "\n",
    "mask = torch.tensor(get_object_mask(img_input[0].cpu()), dtype=torch.float, device=device)\n",
    "mask = mask / torch.max(mask)\n",
    "#The shutter function is encoded into the convolution layer\n",
    "lay = torch.nn.Conv1d(1,1,5)\n",
    "\n",
    "#Manually setting the weights and bias so the  shutter acts as a box filter\n",
    "lay.weight.data = torch.full([1,1,5,1], .2, requires_grad=True, dtype=torch.float, device=device)\n",
    "lay.bias.data = torch.zeros(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "#Target and original class labels\n",
    "target = torch.tensor([targidx], dtype=torch.long, device=device) #Yield\n",
    "orig = torch.tensor([classidx], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 200\n",
    "optimizer = optim.SGD([w], lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targloss = []\n",
    "origloss = []\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Compute g(y) to get X_adv\n",
    "    #gy = lay(ft)    \n",
    "    #w = torch.tanh(gy)\n",
    "    \n",
    "    oot = stack(w,228)\n",
    "    new_w = .5 * (torch.tanh(oot) + 1)\n",
    "    #new_w = .5 * (oot + 1) #Method using Sin, does not work, no gradients\n",
    "    gy = lay(new_w)             #Convolution of ft and the shutter\n",
    "    gy_mask = gy * mask\n",
    "    #gy_mask = gy_mask + (1 - mask)\n",
    "    inp = (c + (1-c)*gy_mask)*img_input         #gy is broadcasted to match the shape of input_im\n",
    "    out = pretrained_model(inp)\n",
    "    #Calculate Loss\n",
    "    loss = loss_fn(out, target)\n",
    "    targloss.append(loss.data)\n",
    "    origloss.append(loss_fn(out,orig))\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "plt.plot(targloss, label=\"target\")\n",
    "plt.plot(origloss, label=\"original\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(targloss[25:], label=\"target\")\n",
    "plt.plot(origloss[25:], label=\"original\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmask = torch.tensor(gy_mask.detach().numpy()[0], device=device)\n",
    "img_out = (1-mask)*img_input + mask*(c+(1-c)*cmask)*img_input\n",
    "print(img_input.size())\n",
    "print(cmask.size())\n",
    "imshow_tensor(img_input[0])\n",
    "imshow_tensor(img_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input.size()"
   ]
  }
 ]
}