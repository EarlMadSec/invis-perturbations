{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def imshow_tensor(img):\n",
    "    img = inv_normalize(img)     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "def saveim(img, name):\n",
    "    img = inv_normalize(img.cpu())\n",
    "    save_image(img, name)\n",
    "def readim(name):\n",
    "    image = Image.open(name)\n",
    "    x = TF.to_tensor(image)\n",
    "    x = forward_normalize(x)\n",
    "    x.unsqueeze_(0)\n",
    "    return x\n",
    "def stack(w,size):\n",
    "    dim = len(torch.flatten(w)) // 3\n",
    "    if dim == size:\n",
    "        return w\n",
    "    ide = torch.eye(dim, requires_grad=True, dtype=torch.float, device=device)\n",
    "    zer = torch.zeros([1,dim], requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "    n = size // dim\n",
    "    m = size % dim\n",
    "    nsum = torch.zeros([1,size,3], requires_grad=True, dtype=torch.float, device=device)\n",
    "    ides = torch.cat(n*[ide])\n",
    "    if m != 0:\n",
    "        zers = torch.cat(m*[zer])\n",
    "        mat = torch.cat([ides,zers])\n",
    "    else:\n",
    "        mat = ides\n",
    "    w2 = w.view([1,dim,3])\n",
    "    nsum = nsum + torch.matmul(mat,w2)\n",
    "    if m == 0:\n",
    "        return nsum.view([3,size,1])\n",
    "    t = []\n",
    "    for i in range(m):\n",
    "        t.append(torch.tensor([1 if x == i else 0 for x in range(dim)],requires_grad=True, dtype=torch.float, device=device))\n",
    "    mat2 = torch.cat([torch.cat(dim*n*[zer]),torch.stack(t)])\n",
    "    nsum = nsum + torch.matmul(mat2,w2)\n",
    "    return nsum.view([3,size,1])\n",
    "def shift_operation(w,offset):\n",
    "    dim = len(torch.flatten(w))//3\n",
    "    ide = torch.eye(dim, device=device)\n",
    "    for i in range(dim):\n",
    "        ide[i][i] = 0\n",
    "        ide[(i+offset)%dim][i] = 1\n",
    "    return torch.matmul(ide,w)\n",
    "def gamma_correction(img, factor):\n",
    "    return ((img+0.5)**factor)-0.5\n",
    "def scale_operation(w, scale):\n",
    "    dim = len(torch.flatten(w))\n",
    "    n_dim = int(dim/scale)\n",
    "    w = w.unsqueeze(0)\n",
    "    t = nn.Upsample(size=(dim,1), mode='bilinear')\n",
    "    n_w = t(w[:,:,:n_dim])\n",
    "    return n_w[0]\n",
    "def split(inp,chan):\n",
    "    size = inp[0].shape\n",
    "    inp2 = copy.deepcopy(inp.detach())\n",
    "    for i in range(len(inp)):\n",
    "        if i != chan:\n",
    "            inp2[i] = torch.zeros(size)\n",
    "    return inp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imstats(name):\n",
    "    x = readim(name)\n",
    "    imshow_tensor(x[0])\n",
    "\n",
    "    prob = lay2(pretrained_model(x.cuda()))\n",
    "    maxcls = prob.max(1)\n",
    "    print(\"Class is {} ({}) with confidence {}%\".format(maxcls.indices.item(),class_dict[maxcls.indices.item()],round(maxcls.values.item()*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "IMG_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/STOP_sign.jpg/1200px-STOP_sign.jpg\"\n",
    "classidx = 919\n",
    "response = requests.get(IMG_URL)\n",
    "img = Image.open(io.BytesIO(response.content))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "forward_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(type(img))\n",
    "img_input = model_transform(img)\n",
    "#img_input = img_input.unsqueeze(0)\n",
    "#img_input = Variable(img_input)\n",
    "img_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = models.resnet101(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "img_input = img_input.to(device)\n",
    "pred = pretrained_model(img_input.unsqueeze(0))\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(pred)\n",
    "maxOcls = prob.max(1)\n",
    "print(\"Class is {} ({}) with confidence {}%\".format(maxOcls.indices.item(),class_dict[maxOcls.indices.item()],maxOcls.values.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_mask(input_image):\n",
    "    #print(test[-3][:])\n",
    "    test = inv_normalize(input_image)\n",
    "    test = np.uint8(test.numpy()*255).transpose((1,2,0))\n",
    "    #test = 1 - test\n",
    "    #test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(test, cv2.COLOR_RGB2GRAY)\n",
    "    th, thgray = cv2.threshold(gray, 75, 255, cv2.THRESH_BINARY_INV); \n",
    "    #blurred = cv2.GaussianBlur(thgray, (9, 9), 0)\n",
    "    th, blurred = cv2.threshold(thgray, 50, 255, cv2.THRESH_BINARY); \n",
    "    edged = cv2.Canny(blurred, 1, 250, L2gradient=True)   \n",
    "    #imgplot = plt.imshow(blurred, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    #imshow(input_image[0])\n",
    "    #applying closing function\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(9,9))\n",
    "    closed = cv2.morphologyEx(blurred, cv2.MORPH_CLOSE, kernel)    #finding_contours\n",
    "    (cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)   \n",
    "    #cnts = [] #only for plate class \n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        cv2.drawContours(closed, [approx], -1, (0, 255, 0), 2)    \n",
    "    th, im_th = cv2.threshold(closed, 254, 255, cv2.THRESH_BINARY_INV);    \n",
    "    h, w = im_th.shape[:2]\n",
    "    im_floodfill = im_th.copy()\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 0);    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)    # Combine the two images to get the foreground.\n",
    "    #imshow(im_th)\n",
    "    #plt.show()\n",
    "    #imshow(im_floodfill_inv)\n",
    "    #plt.show()\n",
    "    im_out = im_th ^ im_floodfill_inv\n",
    "    im_out = cv2.GaussianBlur(im_out, (3, 3), 0)\n",
    "    return im_out\n",
    "print(type(img_input))\n",
    "imshow_tensor(img_input.cpu())\n",
    "imgplot = plt.imshow(get_object_mask(img_input.cpu()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = [598, 697, 898, 567, 69, 855, 641, 600, 942]\n",
    "fold = \"c0.3_sz228_shiftF/\"\n",
    "#targidx = np.random.randint(0,1000)\n",
    "targidx = adv[1]\n",
    "print(targidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input time signal\n",
    "sz = 130\n",
    "c = .5\n",
    "w = torch.rand([3,sz,1], requires_grad=True, dtype=torch.float, device=device)\n",
    "#Does not work since no useful gradients\n",
    "#Aw = torch.rand([1], requires_grad=True, dtype=torch.float, device=device) #amplitude and frequency\n",
    "#sample = torch.linspace(0, 4, sz, dtype=torch.float, device=device)  #Sampling rate, do the math to get this\n",
    "#w = torch.sin(100*Aw[0]*sample).view([1,1,sz,1]) #Sample the sine wave\n",
    "\n",
    "mask = torch.tensor(get_object_mask(img_input.cpu()), dtype=torch.float, device=device)\n",
    "mask = mask / torch.max(mask)\n",
    "#The shutter function is encoded into the convolution layer\n",
    "lay = torch.nn.Conv1d(1,1,5)\n",
    "\n",
    "#Manually setting the weights and bias so the  shutter acts as a box filter\n",
    "lay.weight.data = torch.full([1,1,5,1], .2, requires_grad=True, dtype=torch.float, device=device)\n",
    "lay.bias.data = torch.zeros(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "#Target and original class labels\n",
    "target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "orig = torch.tensor([classidx], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "optimizer = optim.SGD([w], lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targloss = []\n",
    "origloss = []\n",
    "for epoch in tqdm.tqdm_notebook(range(n_epochs)):\n",
    "    half = epoch < n_epochs/2\n",
    "    if epoch == n_epochs/2:\n",
    "        tops = out.topk(2).indices[0]\n",
    "        targidx = tops[0].item() if tops[0].item() != classidx else tops[1].item()\n",
    "        target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "        print(\"Switching from untarget to target {}\".format(targidx))\n",
    "    # Compute g(y) to get X_adv\n",
    "    #print(w.shape)\n",
    "    oot = stack(w,228)\n",
    "    #oot = shift_operation(oot, np.random.randint(0,228))\n",
    "    #oot = scale_operation(oot, np.random.rand(1)[0]+1)\n",
    "    new_w = .5 * (torch.tanh(oot) + 1)\n",
    "    gy = lay(new_w.unsqueeze(0).view([3,1,228,1])).view([1,3,224,1])[0]             #Convolution of ft and the shutter\n",
    "    gy_mask = gy * mask\n",
    "    #gy_mask = gy_mask + (1 - mask)\n",
    "    inp = ((1-mask) + mask*(c + (1-c)*gy_mask))*inv_normalize(img_input)         #gy is broadcasted to match the shape of input_im\n",
    "    inp = forward_normalize(inp)\n",
    "    out = pretrained_model(inp.unsqueeze(0))\n",
    "    #Calculate Loss\n",
    "    if not half: targLoss = loss_fn(out, target)\n",
    "    origLoss = loss_fn(out, orig)\n",
    "    loss = -origLoss if half else targLoss\n",
    "    targloss.append(0 if half else targLoss)\n",
    "    origloss.append(origLoss)\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "plt.plot(targloss, label=\"target\")\n",
    "plt.plot(origloss, label=\"original\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(targloss[n_epochs//2:], label=\"target\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(out)\n",
    "maxcls = prob.max(1)\n",
    "print(prob[0][targidx])\n",
    "print(prob[0][classidx])\n",
    "print(\"Class is {} ({}) with confidence {}%\".format(maxcls.indices.item(),class_dict[maxcls.indices.item()],maxcls.values.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_tensor(img_input.cpu())\n",
    "imshow_tensor(inp.detach().cpu())\n",
    "\n",
    "saveim(img_input, \"original_{}.png\".format(classidx))\n",
    "saveim(inp, \"src{}_c{}_sz{}_tg{}.png\".format(classidx,c,sz,targidx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.ones([3,224,224], device=device)\n",
    "re = test * split(gy_mask,0)\n",
    "gr = test * split(gy_mask,1)\n",
    "bl = test * split(gy_mask,2)\n",
    "imshow_tensor(forward_normalize(re.detach().cpu()))\n",
    "imshow_tensor(forward_normalize(gr.detach().cpu()))\n",
    "imshow_tensor(forward_normalize(bl.detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imstats(\"original_923.png\")\n",
    "for i in adv:\n",
    "    imstats(\"{}adv_{}.png\".format(fold,i))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
