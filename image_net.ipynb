{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display image and predicted class\n",
    "def imstats(name):\n",
    "    # read in image and view it\n",
    "    x = readim(name, forward_normalize) \n",
    "    imshow_tensor(x[0], inv_normalize)\n",
    "\n",
    "    #get predicted class and probability\n",
    "    prob = lay2(pretrained_model(x.cuda()))\n",
    "    maxcls = prob.max(1)\n",
    "    print(\"Class is {} ({}) with confidence {}%\".format(maxcls.indices.item(),class_dict[maxcls.indices.item()],round(maxcls.values.item()*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classes\n",
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "#Retrieve image and assign class\n",
    "IMG_URL = \"https://bloximages.chicago2.vip.townnews.com/chippewa.com/content/tncms/assets/v3/editorial/c/ca/cca0d87d-8d7c-520f-b012-bc065d18b3dd/553cbf5aa0ffb.image.jpg?crop=200%2C200%2C0%2C8&resize=1200%2C1200&order=crop%2Cresize\"\n",
    "classidx = 919\n",
    "response = requests.get(IMG_URL)\n",
    "img = Image.open(io.BytesIO(response.content))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "forward_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(type(img))\n",
    "img_input = model_transform(img)\n",
    "#img_input = img_input.unsqueeze(0)\n",
    "#img_input = Variable(img_input)\n",
    "img_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = models.resnet101(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "#Pass input through model\n",
    "img_input = img_input.to(device)\n",
    "pred = pretrained_model(img_input.unsqueeze(0))\n",
    "\n",
    "#Use softmax to get predicted probability and view it\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(pred)\n",
    "maxOcls = prob.max(1)\n",
    "print(\"Class is {} ({}) with confidence {}%\".format(maxOcls.indices.item(),class_dict[maxOcls.indices.item()],maxOcls.values.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_mask(input_image):\n",
    "    #print(test[-3][:])\n",
    "    test = inv_normalize(input_image)\n",
    "    test = np.uint8(test.numpy()*255).transpose((1,2,0))\n",
    "    test = 1 - test\n",
    "    #test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(test, cv2.COLOR_RGB2GRAY)\n",
    "    th, thgray = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY_INV); \n",
    "    #blurred = cv2.GaussianBlur(thgray, (9, 9), 0)\n",
    "    th, blurred = cv2.threshold(thgray, 70, 255, cv2.THRESH_BINARY)\n",
    "    edged = cv2.Canny(blurred, 1, 250, L2gradient=True)   \n",
    "    #imgplot = plt.imshow(blurred, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    #imshow(input_image[0])\n",
    "    #applying closing function\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    closed = cv2.morphologyEx(blurred, cv2.MORPH_CLOSE, kernel)    #finding_contours\n",
    "    (cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)   \n",
    "    #cnts = [] #only for plate class \n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        cv2.drawContours(closed, [approx], -1, (0, 255, 0), 2)    \n",
    "    th, im_th = cv2.threshold(closed, 254, 255, cv2.THRESH_BINARY_INV);    \n",
    "    h, w = im_th.shape[:2]\n",
    "    im_floodfill = im_th.copy()\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 0);    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)    # Combine the two images to get the foreground.\n",
    "    #imshow(im_th)\n",
    "    #plt.show()\n",
    "    #imshow(im_floodfill_inv)\n",
    "    #plt.show()\n",
    "    im_out = im_th ^ im_floodfill_inv\n",
    "    im_out = cv2.GaussianBlur(im_out, (3, 3), 0)\n",
    "    return im_out\n",
    "print(type(img_input))\n",
    "imshow_tensor(img_input.cpu(),inv_normalize)\n",
    "imgplot = plt.imshow(get_object_mask(img_input.cpu()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing classes. Likely unnecessary\n",
    "adv = [598, 697, 898, 567, 69, 855, 641, 600, 942]\n",
    "fold = \"c0.3_sz228_shiftF/\"\n",
    "#targidx = np.random.randint(0,1000)\n",
    "targidx = adv[1]\n",
    "print(targidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sz = 180   #Length of input signal\n",
    "c = .5     #Ambient light ratio\n",
    "batch = 8\n",
    "# change of variable term to optimise on\n",
    "w = torch.rand([3,sz,1], requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "#Create the mask to only illuminate the object\n",
    "mask = torch.tensor(get_object_mask(img_input.cpu()), dtype=torch.float, device=device)\n",
    "mask = mask / torch.max(mask)\n",
    "\n",
    "#The shutter function is encoded into the convolution layer\n",
    "lay = torch.nn.Conv1d(1,1,5)\n",
    "\n",
    "#Manually setting the weights and bias so the  shutter acts as a box filter\n",
    "lay.weight.data = torch.full([1,1,5,1], .2, requires_grad=True, dtype=torch.float, device=device)\n",
    "lay.bias.data = torch.zeros(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "#Target and original class labels\n",
    "target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "orig = torch.tensor([classidx], dtype=torch.long, device=device)\n",
    "\n",
    "#Model parameters\n",
    "lr = 1e-1\n",
    "n_epochs = 6000\n",
    "optimizer = optim.SGD([w], lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track the loss to target and original class\n",
    "targloss = []\n",
    "origloss = []\n",
    "\n",
    "#Optimisation loop. initially untargeted\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    \n",
    "    #Switch to targeted at halfway point\n",
    "    half = epoch < n_epochs/6\n",
    "    if epoch == n_epochs/6:\n",
    "        tops = out.topk(2).indices[0]\n",
    "        targidx = tops[0].item() if tops[0].item() != classidx else tops[1].item()\n",
    "        target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "        print(\"Switching from untarget to target {}\".format(targidx))\n",
    "        \n",
    "    #Compute g(y) to get X_adv\n",
    "    oot = stack(w,228)             #stack the signal to fit the input size\n",
    "    #oot = shift_operation(oot, np.random.randint(0,sz))\n",
    "    \n",
    "    #oot = scale_operation(oot, np.random.rand(1)[0]+1)\n",
    "    c = torch.rand([batch,1,1,1], device=device) * .5 + .2\n",
    "    shift = torch.randint(0, sz, (batch,))\n",
    "    #print(oot.shape)\n",
    "    ootn = shift_operation(oot.unsqueeze(0).repeat(batch,1,1,1).view(-1, 228, 1), shift).view(batch,3,228,1)\n",
    "    #ootn = torch.cat([shift_operation(oot, i).unsqueeze(0) for i in shift])\n",
    "    #print(ootn.shape)\n",
    "    #Fit w into the range [0,1]. new_w is the same as ft\n",
    "    new_w = .5 * (torch.tanh(ootn) + 1)\n",
    "    \n",
    "    #Convolution of ft and the shutter\n",
    "    #print(new_w.shape)\n",
    "    gy = lay(new_w.unsqueeze(0).view([3,1,228,batch])).view([batch,3,224,1])\n",
    "    #print(gy.shape)\n",
    "    #Mask the signal to only affect the object\n",
    "    gy_mask = gy * mask\n",
    "    \n",
    "    #input calculation:\n",
    "        #combine ambient light with attack signal, and multiply by image\n",
    "        #multiplication must take space in the pre-normalised state, so inverse normalise the input\n",
    "        #afterwards, renormalise to fit the model expectations\n",
    "    #inp = ((1-mask) + mask*(c + (1-c)*gy_mask))*inv_normalize(img_input)   WHY DID WE ASSUME BACKGROUND AT FULL ILLUM?\n",
    "    #print(gy_mask.shape)\n",
    "    inp = (c + (1-c)*gy_mask)*inv_normalize(img_input)   \n",
    "    #print(inp.shape)\n",
    "    inp = torch.cat([forward_normalize(i).unsqueeze(0) for i in inp])\n",
    "    out = pretrained_model(inp)\n",
    "    #print(\"here\")\n",
    "    #Calculate Loss depended on if targeted or untargeted\n",
    "    if not half: targLoss = loss_fn(out, target.repeat(batch))\n",
    "    origLoss = loss_fn(out, orig.repeat(batch))\n",
    "    loss = -origLoss if half else targLoss\n",
    "    targloss.append(0 if half else targLoss)\n",
    "    origloss.append(origLoss)\n",
    "    #print(\"here\")\n",
    "    loss.backward()\n",
    "    del inp\n",
    "    del oot\n",
    "    del ootn\n",
    "    del gy\n",
    "    del gy_mask\n",
    "    del new_w\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "#View original loss and target loss\n",
    "plt.plot(targloss, label=\"target\")\n",
    "plt.plot(origloss, label=\"original\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View target loss after switch to targeted\n",
    "plt.plot(targloss[n_epochs//2:], label=\"target\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create softmax layer to view probabilities\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(out)\n",
    "maxcls = prob.max(1)\n",
    "#print(maxcls)\n",
    "for i in range(batch):\n",
    "    print(\"alpha: {}\".format(c[i].item()))\n",
    "    print(\"target {}: {}%\".format(targidx,prob[i][targidx]))\n",
    "    print(\"Orig {}: {}%\".format(classidx,prob[i][classidx]))\n",
    "    print(\"Class is {} ({}) with confidence {}%\\n\".format(maxcls.indices[i].item(),class_dict[maxcls.indices[i].item()],maxcls.values[i].item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View input image and adversarial image. Save both\n",
    "imshow_tensor(forward_normalize(inv_normalize(img_input.cpu())*c), inv_normalize)\n",
    "imshow_tensor(inp.detach().cpu(),inv_normalize)\n",
    "\n",
    "#saveim(img_input.cpu(), \"original_{}.png\".format(classidx),inv_normalize)\n",
    "#saveim(forward_normalize(inv_normalize(img_input.cpu())*c), \"original_c{}_{}.png\".format(c,classidx),inv_normalize)\n",
    "#saveim(inp, \"src{}_c{}_sz{}_tg{}.png\".format(classidx,c,sz,targidx),inv_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the attack signal colour channels separately\n",
    "test = torch.ones([3,224,224], device=device)\n",
    "re = test * split(gy_mask,0)\n",
    "gr = test * split(gy_mask,1)\n",
    "bl = test * split(gy_mask,2)\n",
    "imshow_tensor(forward_normalize(re.detach().cpu()), inv_normalize)\n",
    "imshow_tensor(forward_normalize(gr.detach().cpu()), inv_normalize)\n",
    "imshow_tensor(forward_normalize(bl.detach().cpu()), inv_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imstats(\"original_919.png\")\n",
    "imstats(\"original_c0.5_919.png\")\n",
    "imstats(\"original_c0.3_919.png\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
