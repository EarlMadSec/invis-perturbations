{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(1)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import image_net_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display image and predicted class\n",
    "def imstats(name):\n",
    "    # read in image and view it\n",
    "    x = readim(name, forward_normalize) \n",
    "    imshow_tensor(x[0], inv_normalize)\n",
    "\n",
    "    #get predicted class and probability\n",
    "    prob = lay2(pretrained_model(x.cuda()))\n",
    "    maxcls = prob.max(1)\n",
    "    print(\"Class is {} ({}) with confidence {}%\".format(maxcls.indices.item(),class_dict[maxcls.indices.item()],round(maxcls.values.item()*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classes\n",
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "#Retrieve image and assign class\n",
    "#IMG_URL = \"https://bloximages.chicago2.vip.townnews.com/chippewa.com/content/tncms/assets/v3/editorial/c/ca/cca0d87d-8d7c-520f-b012-bc065d18b3dd/553cbf5aa0ffb.image.jpg?crop=200%2C200%2C0%2C8&resize=1200%2C1200&order=crop%2Cresize\"\n",
    "#IMG_URL = \"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSa3p29cIlgj0yQwMuLZzh8PKkgBUzbPLWrU-7K79DjDL498JsA&usqp=CAU\"\n",
    "IMG_URL = \"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQ5X7yUzT_3kR7guoqCF5zEFZkesvEkagag8Y1KFJoULsXrL3h0&usqp=CAU\"\n",
    "#IMG_URL = \"https://5.imimg.com/data5/RB/GL/IO/ANDROID-80735891/product-jpeg-500x500.jpg\"\n",
    "classidx = 504\n",
    "response = requests.get(IMG_URL)\n",
    "#img = Image.open(io.BytesIO(response.content))\n",
    "img = Image.open('images/original/col_amb_up.jpg')\n",
    "## for empty signal\n",
    "#img = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "pil_to_tensor = transforms.ToTensor()\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "model_resize = transforms.Resize((224,224))\n",
    "\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "forward_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(type(img))\n",
    "img_input = model_transform(img)\n",
    "#img_input = img_input.unsqueeze(0)\n",
    "#img_input = Variable(img_input)\n",
    "img_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = models.resnet101(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "#Pass input through model\n",
    "img_input = img_input.to(device)\n",
    "pred = pretrained_model(img_input.unsqueeze(0))\n",
    "\n",
    "#Use softmax to get predicted probability and view it\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(pred)\n",
    "maxOcls = prob.max(1)\n",
    "print(\"Class is {} ({}) with confidence {}%\".format(maxOcls.indices.item(),class_dict[maxOcls.indices.item()],maxOcls.values.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(img):\n",
    "    #Pass input through model\n",
    "    img = model_transform(img)\n",
    "    img_input = img.to(device)\n",
    "    pred = pretrained_model(img_input.unsqueeze(0))\n",
    "\n",
    "    #Use softmax to get predicted probability and view it\n",
    "    lay2 = torch.nn.Softmax(dim=1)\n",
    "    prob = lay2(pred)\n",
    "    maxOcls = prob.max(1)\n",
    "    print(\"Class is {} ({}) with confidence {}%\".format(maxOcls.indices.item(),class_dict[maxOcls.indices.item()],maxOcls.values.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_mask(input_image):\n",
    "    #print(test[-3][:])\n",
    "    test = inv_normalize(input_image)\n",
    "    test = np.uint8(test.numpy()*255).transpose((1,2,0))\n",
    "    test = 1 - test\n",
    "    #test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(test, cv2.COLOR_RGB2GRAY)\n",
    "    th, thgray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY); \n",
    "    #blurred = cv2.GaussianBlur(thgray, (9, 9), 0)\n",
    "    th, blurred = cv2.threshold(thgray, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "    edged = cv2.Canny(blurred, 1, 250, L2gradient=True)   \n",
    "    #imgplot = plt.imshow(blurred, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    return thgray\n",
    "    #imshow(input_image[0])\n",
    "    #applying closing function\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    closed = cv2.morphologyEx(blurred, cv2.MORPH_CLOSE, kernel)    #finding_contours\n",
    "    (cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)   \n",
    "    #cnts = [] #only for plate class \n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        cv2.drawContours(closed, [approx], -1, (0, 255, 0), 2)    \n",
    "    th, im_th = cv2.threshold(closed, 254, 255, cv2.THRESH_BINARY_INV);    \n",
    "    h, w = im_th.shape[:2]\n",
    "    im_floodfill = im_th.copy()\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 0);    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)    # Combine the two images to get the foreground.\n",
    "    #imshow(im_th)\n",
    "    #plt.show()\n",
    "    #imshow(im_floodfill_inv)\n",
    "    #plt.show()\n",
    "    im_out = im_th ^ im_floodfill_inv\n",
    "    im_out = cv2.GaussianBlur(im_out, (3, 3), 0)\n",
    "    return im_out\n",
    "print(type(img_input))\n",
    "imshow_tensor(img_input.cpu(),inv_normalize)\n",
    "imgplot = plt.imshow(get_object_mask(img_input.cpu()))\n",
    "plt.show()\n",
    "print(get_object_mask(img_input.cpu()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imcorr(repla):\n",
    "    newrep = torch.zeros_like(repla, dtype=torch.float)\n",
    "    rp = torch.tensor([-1.25006,  2.40351, -0.15585,  0.00132])\n",
    "    gp = torch.tensor([-1.09171,  2.20569, -0.11374, -0.00064])\n",
    "    bp = torch.tensor([-1.04359,  1.84006,  0.21149, -0.00933])\n",
    "    newrep[0] = rp[0] * torch.pow(repla[0], 3) + rp[1] * torch.pow(repla[0], 2) + rp[2] * repla[0] + rp[3]\n",
    "    newrep[1] = gp[0] * torch.pow(repla[1], 3) + gp[1] * torch.pow(repla[1], 2) + gp[2] * repla[1] + gp[3]\n",
    "    newrep[2] = bp[0] * torch.pow(repla[2], 3) + bp[1] * torch.pow(repla[2], 2) + bp[2] * repla[2] + bp[3]\n",
    "    return newrep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img_dir = \"./images/original/mug/\"\n",
    "image_list = [Image.open(img_dir+x) for x in os.listdir(img_dir)]\n",
    "train_image_size = 252\n",
    "image_tensors = torch.cat([\n",
    "                pil_to_tensor(\n",
    "                    x.resize((train_image_size,train_image_size)))\n",
    "                     .unsqueeze(0) for x in image_list])\n",
    "image_tensors = image_tensors.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial(margin):\n",
    "    x = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    out = []\n",
    "    for i in range(3):\n",
    "        y = [max(0, xi + random.random()*2*margin - margin) for xi in x]\n",
    "        out.append(np.polyfit(x,y,3))\n",
    "    return out\n",
    "def imcorr2(repla, coeff):\n",
    "    newrep = torch.zeros_like(repla, dtype=torch.float)\n",
    "    rp = coeff[0]\n",
    "    gp = coeff[1]\n",
    "    bp = coeff[2]\n",
    "    newrep[0] = rp[0] * torch.pow(repla[0], 3) + rp[1] * torch.pow(repla[0], 2) + rp[2] * repla[0] + rp[3]\n",
    "    newrep[1] = gp[0] * torch.pow(repla[1], 3) + gp[1] * torch.pow(repla[1], 2) + gp[2] * repla[1] + gp[3]\n",
    "    newrep[2] = bp[0] * torch.pow(repla[2], 3) + bp[1] * torch.pow(repla[2], 2) + bp[2] * repla[2] + bp[3]\n",
    "    return newrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "y = [max(0, xi + random.random()*2*0.2 - 0.2) for xi in x]\n",
    "coeffs = np.polyfit(x,y,3)[::-1]\n",
    "print(coeffs[::-1])\n",
    "y = np.array([np.sum(np.array([coeffs[i]*(j**i) for i in range(len(coeffs))])) for j in x])\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_batch(batch_size, train_image_size):\n",
    "    selected_images = indices = torch.randperm(len(image_list))[:batch_size]\n",
    "    return image_tensors[selected_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_resize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_full = Image.open('images/original/col_full_up.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(image_net_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_list = [\n",
    "    {\n",
    "        'device':device,\n",
    "        'exposure':2000,\n",
    "        'batch_size':32,\n",
    "        'image_cache_size':1,\n",
    "        'original_images':[img, img_full],\n",
    "        'number_test_images':1,\n",
    "        'n_epochs':1,\n",
    "        'classes_to_skip':[504, 901, 968, 647,899,725,441,505,441,969,438,967],\n",
    "        'classidx':504,\n",
    "        'model_img_size':252,\n",
    "        'targidx':722,\n",
    "        'apply_transformations':False,\n",
    "        'ambient_light':1,\n",
    "        'color_noise':None\n",
    "    },\n",
    "    {\n",
    "        'device':device,\n",
    "        'exposure':2000,\n",
    "        'batch_size':32,\n",
    "        'image_cache_size':1,\n",
    "        'original_images':[img, img_full],\n",
    "        'number_test_images':1,\n",
    "        'n_epochs':10000,\n",
    "        'classes_to_skip':[504, 901, 968, 647,899,725,441,505,441,969,438,967],\n",
    "        'classidx':504,\n",
    "        'model_img_size':252,\n",
    "        'targidx':722,\n",
    "        'apply_transformations':False,\n",
    "        'ambient_light':1,\n",
    "        'color_noise':None\n",
    "    },\n",
    "    {\n",
    "        'device':device,\n",
    "        'exposure':2000,\n",
    "        'batch_size':32,\n",
    "        'image_cache_size':1,\n",
    "        'original_images':[img, img_full],\n",
    "        'number_test_images':1,\n",
    "        'n_epochs':10000,\n",
    "        'classes_to_skip':[504, 901, 968, 647,899,725,441,505,441,969,438,967],\n",
    "        'classidx':504,\n",
    "        'model_img_size':252,\n",
    "        'targidx':722,\n",
    "        'apply_transformations':False,\n",
    "        'ambient_light':1,\n",
    "        'color_noise':\"gy_noise\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_values = []\n",
    "for a in range(len(arg_list)):\n",
    "    args = arg_list[a]\n",
    "    args_in_list = []\n",
    "    for v in args.values(): args_in_list.append(v)\n",
    "    args_in_list.append(args)\n",
    "    max_acc, class_acc, target_acc, targloss, origloss, targidx = image_net_python.run_simulation(*args_in_list)\n",
    "    '''\n",
    "    #View original loss and target loss\n",
    "    plt.plot(targloss, label=\"target\")\n",
    "    plt.plot(origloss, label=\"original\")\n",
    "    plt.xlabel('no of iterations/10',fontsize=13)\n",
    "    plt.ylabel('loss',fontsize=13)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    chart = sns.countplot([class_dict[x[0]][:15] for x in max_acc])\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "    '''\n",
    "    avg_class_acc = np.array(class_acc).mean()\n",
    "    avg_target_acc = np.array(target_acc).mean()\n",
    "    dodge_success = np.array([1 if x[0] not in args['classes_to_skip'] else 0 for x in max_acc]).mean()\n",
    "    target_success = np.array([1 if x[0]==targidx else 0 for x in max_acc]).mean()\n",
    "    output_classes = {}\n",
    "    for cl in max_acc: \n",
    "        if cl[0] not in output_classes: output_classes[cl[0]]=0\n",
    "        output_classes[cl[0]]+=1\n",
    "    print(\"Avg. accuracy for {} (original) - {}\".format(class_dict[args['classidx']],avg_class_acc))\n",
    "    print(\"Avg. accuracy for {} (target)- {}\".format(class_dict[targidx],avg_target_acc))\n",
    "    print(\"Dodge success outside classes to skip - {}\".format(dodge_success))\n",
    "    print(\"Target success - {}\".format(target_success))\n",
    "    print(\"Output class histogram - \",output_classes)\n",
    "    output_values.append((avg_class_acc,avg_target_acc,dodge_success,target_success,output_classes))\n",
    "\n",
    "output_values_config = [\"Class acc\", \"Target acc\", \"Dodge(similar)\",\"Targeted\"]\n",
    "input_configs = [\n",
    "    {'base':'case'},\n",
    "    {'noise':\"No\"},\n",
    "    {'noise':\"gy_noise\"}\n",
    "]\n",
    "\n",
    "df_list = []\n",
    "for i,inpc in enumerate(input_configs):\n",
    "    config_name = \",\".join([\"{}-{}\".format(k,v) for k,v in inpc.items()])\n",
    "    for j in range(4):\n",
    "        df_list.append([config_name, output_values[i][j], output_values_config[j]])\n",
    "df_results = pd.DataFrame(df_list, columns = ['config','metric_value','metric_type'])\n",
    "\n",
    "plot_title = 'Different noise - 10k, no transformation'\n",
    "sns.lineplot(data=df_results, x='config', y='metric_value',hue=\"metric_type\",sort=False).set_title(plot_title)\n",
    "plt.xticks(rotation=30)\n",
    "#plt.savefig('plots/'+plot_title+'.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(img_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_transform(image1, image2, brightness_multiplier=None):\n",
    "        # Adjust Brightness\n",
    "        if brightness_multiplier==None:\n",
    "            brightness_multiplier = 1 + random.random()\n",
    "        imageb = TF.adjust_brightness(image1, brightness_multiplier)\n",
    "    \n",
    "        # Resize\n",
    "        re_size = random.randint(252,360)\n",
    "        resize = transforms.Resize(re_size)\n",
    "        image1 = resize(image1)\n",
    "        image2 = resize(image2)\n",
    "        imageb = resize(imageb)\n",
    "\n",
    "        # Random crop\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(\n",
    "            image1, output_size=(252, 252))\n",
    "        image1 = TF.crop(image1, i, j, h, w)\n",
    "        image2 = TF.crop(image2, i, j, h, w)\n",
    "        imageb = TF.crop(imageb, i, j, h, w)\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image1 = TF.hflip(image1)\n",
    "            image2 = TF.hflip(image2)\n",
    "            imageb = TF.hflip(imageb)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image1 = TF.vflip(image1)\n",
    "            image2 = TF.vflip(image2)\n",
    "            imageb = TF.vflip(imageb)\n",
    "            \n",
    "        # Random Rotation\n",
    "        rotate = transforms.RandomRotation(45, resample=Image.BILINEAR)\n",
    "        seed = random.randint(0,2**32)\n",
    "        random.seed(seed)\n",
    "        image1 = rotate(image1)\n",
    "        random.seed(seed)\n",
    "        image2 = rotate(image2)\n",
    "        random.seed(seed)\n",
    "        imageb = rotate(imageb)\n",
    "\n",
    "        return image1, imageb, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_temp, img_br, img_full_temp = aug_transform(img, img_full, 1.5)\n",
    "imshow(img_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(img_full_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction(img_full_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize2d(img, size):\n",
    "        return (F.adaptive_avg_pool2d(Variable(img), size)).data\n",
    "def upsample2d(img, size=224):\n",
    "    upsample = nn.Upsample(size=size, mode='bilinear', align_corners=False)\n",
    "    return upsample(torch.unsqueeze(img, 0))[0]\n",
    "\n",
    "def get_image_pair(img, img_full):\n",
    "    repeat_size = int(3024/3024)\n",
    "    img = img.resize((252,252))\n",
    "    model_img_size = img.size[0]\n",
    "    img_t, img_f = aug_transform(img, img_full)\n",
    "    img_t = pil_to_tensor(img_t)\n",
    "    img_t = img_t.to(device)\n",
    "    img_f = pil_to_tensor(img_f.resize((252,252))).to(device)\n",
    "    return img_t, img_f\n",
    "\n",
    "def get_image_trip(img, img_full):\n",
    "    repeat_size = int(3024/3024)\n",
    "    img = img.resize((252,252))\n",
    "    model_img_size = img.size[0]\n",
    "    img_t, img_b, img_f = aug_transform(img, img_full, 1)\n",
    "    img_t = pil_to_tensor(img_t)\n",
    "    img_t = img_t.to(device)\n",
    "    img_b = pil_to_tensor(img_b).to(device)\n",
    "    img_f = pil_to_tensor(img_f.resize((252,252))).to(device)\n",
    "    return img_t, img_b, img_f\n",
    "\n",
    "#For resize post convolution\n",
    "if account_resize:\n",
    "    repeat_size = int(3024/3024)\n",
    "    img = img.resize((252,252))\n",
    "    model_img_size = img.size[0]\n",
    "    img_t = pil_to_tensor(img)\n",
    "    img_t = img_t.to(device)\n",
    "    img_f = pil_to_tensor(img_full.resize((252,252))).to(device)\n",
    "else:\n",
    "    model_img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_size = 150\n",
    "image_cache = []\n",
    "for i in range(cache_size):\n",
    "    image_cache.append(get_image_trip(img, img_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical exposure is in form 1/n s. Available: 15, 20, 25 30, 40, 50, 60, 80, 100, 125, 160, 200, 250\n",
    "exposure = 500 \n",
    "exp_micros = 1000000/exposure          # get exposure in microseconds\n",
    "img_ratio = 3024 / model_img_size      # every row in model is img_ratio rows in original image\n",
    "model_tr = 10 * img_ratio              # multiply real tr (10 micros) by img_ratio to find model tr\n",
    "conv_size = exp_micros / model_tr      # divide exposure time by tr to find convolution size\n",
    "conv_size = int(conv_size)             # Need closest integer approximation. Won't cause a significant difference\n",
    "conv_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = model_img_size + conv_size - 1 # 300   #Length of input signal\n",
    "c = 0    #Ambient light ratio\n",
    "c_limits = [0,0]\n",
    "batch = 16\n",
    "channels = 3\n",
    "# change of variable term to optimise on\n",
    "w = torch.rand([channels,sz,1], requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "#Create the mask to only illuminate the object\n",
    "mask = torch.tensor(get_object_mask(img_input.cpu()), dtype=torch.float, device=device)\n",
    "mask = mask / torch.max(mask)\n",
    "\n",
    "\n",
    "#Target and original class labels\n",
    "orig = torch.tensor([classidx], dtype=torch.long, device=device)\n",
    "\n",
    "#Model parameters\n",
    "lr = 1e-1\n",
    "n_epochs = 5\n",
    "#optimizer = optim.SGD([w], lr=lr, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam([w], lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "classes_to_skip = [classidx, 901, 968, 647,899,725,441,505,441,969,438,967]\n",
    "targidx = 722\n",
    "target = torch.tensor([targidx], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track the loss to target and original class\n",
    "targloss = []\n",
    "origloss = []\n",
    "\n",
    "#obj_dict = {}\n",
    "\n",
    "#Optimisation loop. initially untargeted\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    \n",
    "    #Switch to targeted at halfway point\n",
    "    half = epoch < n_epochs//6\n",
    "    if epoch == n_epochs//6:\n",
    "        tops = out.topk(40).indices[0]\n",
    "        for t in tops:\n",
    "            if t.item() not in classes_to_skip: \n",
    "                targidx = t.item()\n",
    "        #targidx = 722\n",
    "        #targidx = tops[0].item() if tops[0].item() != classidx else tops[1].item()\n",
    "        target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "        print(\"Switching from untarget to target {}\".format(targidx))\n",
    "    \n",
    "    if channels==1:\n",
    "        n_w = w.repeat(3,1,1)\n",
    "    else:\n",
    "        n_w = w\n",
    "    \n",
    "    # For resize post convolution\n",
    "    if account_resize:\n",
    "        n_w = torch.repeat_interleave(n_w, repeats=repeat_size, dim=1)\n",
    "    \n",
    "    sig_height = model_img_size + conv_size - 1\n",
    "    gy, new_w, sh = fttogy(n_w, batch, None, c_limits, sig_height, conv_size, device, 0, shifting=True)\n",
    "    #display(tensor_to_pil(gy[0]))\n",
    "    #color correction\n",
    "    #gy = torch.cat([imcorr(i).unsqueeze(0) for i in gy])\n",
    "    \n",
    "    #gamma correction\n",
    "    #gy = torch.pow(gy, 1/2.2)\n",
    "    \n",
    "    if account_resize:\n",
    "        #For resize post convolution\n",
    "        #inp = gy*img_t\n",
    "        #gy *= random.random()*0.7 + 0.7\n",
    "        \n",
    "        #Add noise to input signal\n",
    "        #xx = 0.4*torch.rand((gy.size()[0],gy.size()[1]),device=device)-0.2\n",
    "        #xx = xx.unsqueeze(2).unsqueeze(3).repeat(1,1,252,1)\n",
    "        #gy += xx\n",
    "        #gy = torch.clamp(gy, min=0, max=1)\n",
    "        \n",
    "        # without brightness random\n",
    "        #img_t, img_f = image_pair_cache[random.randint(0,cache_size-1)]\n",
    "        #inp = torch.pow(0.0000001 + torch.pow(img_t,2.2) + gy*(torch.pow(img_f,2.2)-torch.pow(img_t,2.2)), 1/2.2)\n",
    "        \n",
    "        img_amb, img_bright, img_f = img_t, img_t, img_f#image_cache[random.randint(0,cache_size-1)]\n",
    "        inp = torch.pow(0.0000001 + torch.pow(img_bright,2.2) + gy*(torch.pow(img_f,2.2)-torch.pow(img_amb,2.2)), 1/2.2)\n",
    "        \n",
    "        #For multiple train images\n",
    "        #inp = gy*get_image_batch(batch, 252)\n",
    "    else: \n",
    "        inp = gy*inv_normalize(img_input)\n",
    "    \n",
    "    #Gaussian Noise\n",
    "    #inp = inp + torch.randn(inp.size(),device=device)*0.005\n",
    "    #inp = torch.clamp(inp,min=0,max=1)\n",
    "    \n",
    "    #Uniform noise to each row\n",
    "    #inp = inp*(1 + torch.rand(inp.size()[:-1],device=device).unsqueeze(-1)*0.4 - 0.2)\n",
    "    #inp = torch.clamp(inp,min=0,max=1)\n",
    "    \n",
    "    #apply random color correction\n",
    "    #inp = torch.cat([imcorr2(i, get_polynomial(0.2)).unsqueeze(0) for i in inp])\n",
    "    \n",
    "    if account_resize:\n",
    "        #For resize post convolution\n",
    "        inp = torch.cat([forward_normalize(upsample2d(i,224)).unsqueeze(0) for i in inp])\n",
    "    else: \n",
    "        inp = torch.cat([forward_normalize(i).unsqueeze(0) for i in inp])\n",
    "    inp = inp.to(device)\n",
    "    out = pretrained_model(inp)\n",
    "\n",
    "    #Calculate Loss depended on if targeted or untargeted\n",
    "    if not half: targLoss = loss_fn(out, target.repeat(batch))\n",
    "    origLoss = loss_fn(out, orig.repeat(batch))\n",
    "    loss = -origLoss if half else targLoss\n",
    "    if epoch%10 == 0:\n",
    "        targloss.append(0 if half else targLoss)\n",
    "        origloss.append(origLoss)\n",
    "    if epoch%1000 == 0:\n",
    "        if not half: print(targLoss, origLoss) \n",
    "        else: print(origLoss)\n",
    "    loss.backward()   \n",
    "    \n",
    "    if epoch%10==0:   # batch 32 === update after 10 for ~224\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        del loss\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if epoch!=n_epochs-1:\n",
    "        del inp\n",
    "        del new_w\n",
    "    #else:\n",
    "        #saving w to be used for prediction\n",
    "        #torch.save(n_w,'w_0.5_764.pt')\n",
    "\n",
    "    #Code to check gpu allocation    \n",
    "    '''\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                #print(type(obj), obj.size())\n",
    "                if type(obj) not in obj_dict:\n",
    "                    obj_dict[type(obj)] = 1\n",
    "                else:\n",
    "                    obj_dict[type(obj)] += 1\n",
    "        except: pass\n",
    "    print(obj_dict)\n",
    "    obj_dict.clear()\n",
    "    '''\n",
    "    \n",
    "#View original loss and target loss\n",
    "plt.plot(targloss, label=\"target\")\n",
    "plt.plot(origloss, label=\"original\")\n",
    "plt.xlabel('no of iterations/10',fontsize=13)\n",
    "plt.ylabel('loss',fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create softmax layer to view probabilities\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "#targidx = 4\n",
    "prob = lay2(out)\n",
    "maxcls = prob.max(1)\n",
    "#print(maxcls)\n",
    "for i in range(batch):\n",
    "    #print(\"alpha: {}\".format(c[i].item()))\n",
    "    print(\"target {}: {}%\".format(targidx,prob[i][targidx]))\n",
    "    print(\"Orig {}: {}%\".format(classidx,prob[i][classidx]))\n",
    "    print(\"Class is {} ({}) with confidence {}%\\n\".format(maxcls.indices[i].item(),class_dict[maxcls.indices[i].item()],maxcls.values[i].item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classidx = 504\n",
    "print('source :'+str(np.array([float(prob[i][classidx]) for i in range(batch) if prob.max(1).indices[i] == classidx]).mean()))\n",
    "print('target :'+str(np.array([float(prob[i][722]) for i in range(batch)]).mean()))\n",
    "print('max :'+str(np.array([float(prob[i][prob.max(1).indices[i].item()]) for i in range(batch)]).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(new_w,'saved_inputs/imagenet_iso800_504_targeted722_rgb_resize252_spatialtransforms.pt')\n",
    "test = torch.ones([3,252,252])\n",
    "imshow_tensor(forward_normalize(gy[15].detach().cpu()*test), inv_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View input image and adversarial image. Save both\n",
    "#imshow_tensor(forward_normalize(inv_normalize(img_input.cpu())*c), inv_normalize)\n",
    "imshow_tensor(inp[5].detach().cpu(),inv_normalize)\n",
    "imshow_tensor(inp[10].detach().cpu(),inv_normalize)\n",
    "imshow_tensor(inp[15].detach().cpu(),inv_normalize)\n",
    "#imshow_tensor(inp[20].detach().cpu(),inv_normalize)\n",
    "#imshow_tensor(inp[25].detach().cpu(),inv_normalize)\n",
    "#saveim(img_input.cpu(), \"original_{}.png\".format(classidx),inv_normalize)\n",
    "#saveim(forward_normalize(inv_normalize(img_input.cpu())*c), \"original_c{}_{}.png\".format(c,classidx),inv_normalize)\n",
    "#saveim(inp, \"src{}_c{}_sz{}_tg{}.png\".format(classidx,c,sz,targidx),inv_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.flatten(new_w[0][0]).detach().cpu(), label=\"w_r\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(torch.flatten(new_w[0][1]).detach().cpu(), label=\"w_g\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(torch.flatten(new_w[0][2]).detach().cpu(), label=\"w_b\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate for different augmented images\n",
    "class_acc, target_acc, max_acc = [],[],[]\n",
    "obj_dict = {}\n",
    "for i in range(1):\n",
    "    # without random bright\n",
    "    #img_t2, img_f2 = get_image_pair(img, img_full)\n",
    "    #inp2 = torch.pow(0.0000001 + torch.pow(img_t2,2.2) + gy*(torch.pow(img_f2,2.2)-torch.pow(img_t2,2.2)), 1/2.2)\n",
    "    \n",
    "    img_t2, img_b2, img_f2 = img_t, img_t, img_f#get_image_trip(img, img_full)\n",
    "    inp2 = torch.pow(0.0000001 + torch.pow(img_b2,2.2) + gy*(torch.pow(img_f2,2.2)-torch.pow(img_t2,2.2)), 1/2.2)\n",
    "    \n",
    "    inp2 = torch.cat([forward_normalize(upsample2d(i,224)).unsqueeze(0) for i in inp2])\n",
    "    inp2 = inp2.to(device)\n",
    "    out2 = pretrained_model(inp2)\n",
    "    prob2 = lay2(out2)\n",
    "    maxcls2 = prob2.max(1)\n",
    "    #print(maxcls)\n",
    "    for i in range(batch):\n",
    "        #print(\"alpha: {}\".format(c[i].item()))\n",
    "        #print(\"target {}: {}%\".format(targidx,prob2[i][targidx]))\n",
    "        target_acc.append(prob2[i][targidx].item())\n",
    "        class_acc.append(prob2[i][classidx].item())\n",
    "        #print(\"Orig {}: {}%\".format(classidx,prob2[i][classidx]))\n",
    "        max_acc.append((maxcls2.indices[i].item(), prob2[i][maxcls2.indices[i].item()].item()))\n",
    "        #print(\"Class is {} ({}) with confidence {}%\\n\".format(maxcls2.indices[i].item(),class_dict[maxcls2.indices[i].item()],maxcls2.values[i].item()*100))\n",
    "    del img_t2\n",
    "    del img_f2\n",
    "    del inp2\n",
    "    del out2\n",
    "    del prob2\n",
    "    del maxcls2\n",
    "    torch.cuda.empty_cache()\n",
    "plt.figure(figsize=(10,5))\n",
    "chart = sns.countplot([class_dict[x[0]][:15] for x in max_acc])\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate for different augmented images without signal\n",
    "class_acc, target_acc, max_acc = [],[],[]\n",
    "obj_dict = {}\n",
    "for i in range(50):\n",
    "    img_t2, img_f2 = get_image_pair(img, img_full)\n",
    "    inp2 = forward_normalize(upsample2d(img_t2,224)).unsqueeze(0)\n",
    "    inp2 = inp2.to(device)\n",
    "    out2 = pretrained_model(inp2)\n",
    "    prob2 = lay2(out2)\n",
    "    maxcls2 = prob2.max(1)\n",
    "    \n",
    "    target_acc.append(prob2[0][targidx].item())\n",
    "    class_acc.append(prob2[0][classidx].item())\n",
    "    #print(\"Orig {}: {}%\".format(classidx,prob2[i][classidx]))\n",
    "    max_acc.append((maxcls2.indices[0].item(), prob2[0][maxcls2.indices[0].item()].item()))\n",
    "    #print(\"Class is {} ({}) with confidence {}%\\n\".format(maxcls2.indices[i].item(),class_dict[maxcls2.indices[i].item()],maxcls2.values[i].item()*100))\n",
    "    del img_t2\n",
    "    del img_f2\n",
    "    del inp2\n",
    "    del out2\n",
    "    del prob2\n",
    "    del maxcls2\n",
    "    torch.cuda.empty_cache()\n",
    "plt.figure(figsize=(5,5))\n",
    "chart = sns.countplot([class_dict[x[0]][:15] for x in max_acc])\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imstats(\"original_919.png\")\n",
    "imstats(\"original_c0.5_919.png\")\n",
    "imstats(\"original_c0.3_919.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classes\n",
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "#Retrieve image and assign class\n",
    "IMG_URL = \"https://bloximages.chicago2.vip.townnews.com/chippewa.com/content/tncms/assets/v3/editorial/c/ca/cca0d87d-8d7c-520f-b012-bc065d18b3dd/553cbf5aa0ffb.image.jpg?crop=200%2C200%2C0%2C8&resize=1200%2C1200&order=crop%2Cresize\"\n",
    "classidx = 919\n",
    "response = requests.get(IMG_URL)\n",
    "img = Image.open(io.BytesIO(response.content))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = 5\n",
    "test = torch.tensor([[0., 1., 1., 1., 1.],\n",
    "        [2., 2., 2., 2., 2.],\n",
    "        [3., 3., 3., 3., 3.]],device=device)\n",
    "print(test)\n",
    "test_batch = 2\n",
    "test_shifted = shift_operation(test.unsqueeze(0).repeat(test_batch,1,1,1).view(-1, test_length, 1), [0,1]).view(test_batch,3,test_length,1)\n",
    "print(test_shifted.shape)\n",
    "print(test_shifted)\n",
    "test_reshaped = test_shifted.view([3,1,test_length,test_batch])\n",
    "treshape = test_shifted.transpose(0,3).transpose(0,1)\n",
    "print(test_reshaped.shape)\n",
    "print(test_reshaped)\n",
    "print(treshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((8,8),dtype=torch.float)\n",
    "import math\n",
    "pi = math.pi\n",
    "print(a)\n",
    "print(torch.round(a*45)/45)\n",
    "print(torch.mean(torch.abs(a - torch.round(a*45)/45)))\n",
    "b = a-torch.sin(90*pi*a)/(90*pi)\n",
    "print(b)\n",
    "print(torch.mean(torch.abs(b - torch.round(a*45)/45)))\n",
    "c = b-torch.sin(90*pi*b)/(90*pi)\n",
    "print(c)\n",
    "print(torch.mean(torch.abs(c - torch.round(a*45)/45)))\n",
    "d = c-torch.sin(90*pi*c)/(90*pi)\n",
    "print(d)\n",
    "print(torch.mean(torch.abs(d - torch.round(a*45)/45)))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
