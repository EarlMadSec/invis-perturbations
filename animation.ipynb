{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classes\n",
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "#Retrieve image and assign class\n",
    "#IMG_URL = \"https://bloximages.chicago2.vip.townnews.com/chippewa.com/content/tncms/assets/v3/editorial/c/ca/cca0d87d-8d7c-520f-b012-bc065d18b3dd/553cbf5aa0ffb.image.jpg?crop=200%2C200%2C0%2C8&resize=1200%2C1200&order=crop%2Cresize\"\n",
    "#IMG_URL = \"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSa3p29cIlgj0yQwMuLZzh8PKkgBUzbPLWrU-7K79DjDL498JsA&usqp=CAU\"\n",
    "#IMG_URL = \"https://thumbs-prod.si-cdn.com/D9GA8QHJQ70nFNQB-AFZ41S-Bs0=/fit-in/1600x0/https://public-media.si-cdn.com/filer/ea/21/ea2159df-9eec-4b05-a3de-356c57e23227/another_airplane_4676723312.jpg\"\n",
    "IMG_URL = \"https://5.imimg.com/data5/OX/QV/SF/SELLER-23061093/national-35-model-air-rifle-500x500.jpg\"\n",
    "#classidx = 919\n",
    "classidx = 504\n",
    "response = requests.get(IMG_URL)\n",
    "img = Image.open('images/original/test.jpg')\n",
    "#img = Image.new('RGB', (3024, 3024), (255, 255, 255))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "pil_to_tensor = transforms.ToTensor()\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "model_resize = transforms.Resize((224,224))\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "forward_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(type(img))\n",
    "img_input = model_transform(img)\n",
    "#img_input = img_input.unsqueeze(0)\n",
    "#img_input = Variable(img_input)\n",
    "img_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = models.resnet101(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "#Pass input through model\n",
    "img_input = img_input.to(device)\n",
    "pred = pretrained_model(img_input.unsqueeze(0))\n",
    "\n",
    "#Use softmax to get predicted probability and view it\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(pred)\n",
    "maxOcls = prob.max(1)\n",
    "print(\"Class is {} ({}) with confidence {}%\".format(maxOcls.indices.item(),class_dict[maxOcls.indices.item()],maxOcls.values.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_mask(input_image):\n",
    "    #print(test[-3][:])\n",
    "    test = inv_normalize(input_image)\n",
    "    test = np.uint8(test.numpy()*255).transpose((1,2,0))\n",
    "    test = 1 - test\n",
    "    #test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(test, cv2.COLOR_RGB2GRAY)\n",
    "    th, thgray = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY_INV); \n",
    "    #blurred = cv2.GaussianBlur(thgray, (9, 9), 0)\n",
    "    th, blurred = cv2.threshold(thgray, 70, 255, cv2.THRESH_BINARY_INV)\n",
    "    edged = cv2.Canny(blurred, 1, 250, L2gradient=True)   \n",
    "    #imgplot = plt.imshow(blurred, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    return blurred\n",
    "    #imshow(input_image[0])\n",
    "    #applying closing function\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    closed = cv2.morphologyEx(blurred, cv2.MORPH_CLOSE, kernel)    #finding_contours\n",
    "    (cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)   \n",
    "    #cnts = [] #only for plate class \n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        cv2.drawContours(closed, [approx], -1, (0, 255, 0), 2)    \n",
    "    th, im_th = cv2.threshold(closed, 254, 255, cv2.THRESH_BINARY_INV);    \n",
    "    h, w = im_th.shape[:2]\n",
    "    im_floodfill = im_th.copy()\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 0);    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)    # Combine the two images to get the foreground.\n",
    "    #imshow(im_th)\n",
    "    #plt.show()\n",
    "    #imshow(im_floodfill_inv)\n",
    "    #plt.show()\n",
    "    im_out = im_th ^ im_floodfill_inv\n",
    "    im_out = cv2.GaussianBlur(im_out, (3, 3), 0)\n",
    "    return im_out\n",
    "print(type(img_input))\n",
    "imshow_tensor(img_input.cpu(),inv_normalize)\n",
    "#imm = Image.fromarray(get_object_mask(img_input.cpu()))\n",
    "#imm.save(\"class919_mask.png\")\n",
    "imgplot = plt.imshow(get_object_mask(img_input.cpu()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imcorr(repla):\n",
    "    newrep = torch.zeros_like(repla, dtype=torch.float,device=device)\n",
    "    rp = torch.tensor([-1.25006,  2.40351, -0.15585,  0.00132],device=device)\n",
    "    gp = torch.tensor([-1.09171,  2.20569, -0.11374, -0.00064],device=device)\n",
    "    bp = torch.tensor([-1.04359,  1.84006,  0.21149, -0.00933],device=device)\n",
    "    newrep[0] = rp[0] * torch.pow(repla[0], 3) + rp[1] * torch.pow(repla[0], 2) + rp[2] * repla[0] + rp[3]\n",
    "    newrep[1] = gp[0] * torch.pow(repla[1], 3) + gp[1] * torch.pow(repla[1], 2) + gp[2] * repla[1] + gp[3]\n",
    "    newrep[2] = bp[0] * torch.pow(repla[2], 3) + bp[1] * torch.pow(repla[2], 2) + bp[2] * repla[2] + bp[3]\n",
    "    return newrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor(get_object_mask(img_input.cpu()), dtype=torch.float, device=device)\n",
    "mask = mask / torch.max(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMask(new_w, batch_limits, mask, c_limits, sig_height, conv_size, precision_depth=2):\n",
    "    \n",
    "    lay = torch.nn.Conv1d(1,1,conv_size)\n",
    "\n",
    "    #Manually setting the weights and bias so the  shutter acts as a box filter\n",
    "    lay.weight.data = torch.full([1,1,conv_size,1], 1/conv_size, requires_grad=True, dtype=torch.float, device=device)\n",
    "    lay.bias.data = torch.zeros(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "    \n",
    "    sz = new_w.shape[1]\n",
    "    batch = batch_limits[1]-batch_limits[0]\n",
    "    #stack the signal to fit the input size\n",
    "    new_w = stack(new_w,sig_height)             \n",
    "    \n",
    "    # EOT sampling for ambient light and shift\n",
    "    c = torch.rand([batch,1,1,1], device=device) * (c_limits[1] - c_limits[0]) + c_limits[0]\n",
    "    shift = torch.tensor(range(batch_limits[0],batch_limits[1]), dtype=torch.int)\n",
    "    #shift = torch.from_numpy(np.array(range(0,batch,1)))\n",
    "    \n",
    "    #Shift the signal\n",
    "    new_w = shift_operation(new_w.unsqueeze(0).repeat(batch,1,1,1).view(-1, sig_height, 1), shift).view(batch,3,sig_height,1)\n",
    "    \n",
    "    #Fit w into the range [0,1]. new_w is the same as ft\n",
    "    #new_w = .5 * (torch.tanh(ootn) + 1)\n",
    "    \n",
    "    #precision limit\n",
    "    new_w = diff_round(new_w, precision_depth)\n",
    "    \n",
    "    #Convolution of ft and the shutter\n",
    "    #gy = lay(new_w.unsqueeze(0).view([3,1,228,batch])).view([batch,3,224,1])\n",
    "    gy = lay(new_w.transpose(0,3).transpose(0,1)).transpose(0,1).transpose(0,3)\n",
    "\n",
    "    #Mask the signal to only affect the object\n",
    "    if mask:\n",
    "        gy_mask = torch.mul(gy,torch.transpose(mask,1,0))\n",
    "    else:\n",
    "        gy_mask = gy\n",
    "\n",
    "    return (c + (1-c)*gy_mask), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"./images/original/mug/\"\n",
    "#image_list = [Image.open(img_dir+x) for x in os.listdir(img_dir)]\n",
    "image_list = [Image.open('./images/original/mugfinal.jpg')]\n",
    "train_image_size = 252\n",
    "image_tensors = torch.cat([\n",
    "                pil_to_tensor(\n",
    "                    x.resize((train_image_size,train_image_size)))\n",
    "                     .unsqueeze(0) for x in image_list])\n",
    "image_tensors = image_tensors.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_batch(batch_size, index=None):\n",
    "    selected_images = torch.randperm(len(image_list))[:batch_size]\n",
    "    if index!=None:\n",
    "        return image_tensors[selected_images]\n",
    "    else:\n",
    "        if len(index)<batch_size:\n",
    "            index += random.sample(range(0, len(image_list)), len(image_list)-len(index))\n",
    "        return image_tensors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_resize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For resize post convolution\n",
    "if account_resize:\n",
    "    repeat_size = int(252/252)\n",
    "    img_n = img.resize((252,252))\n",
    "    model_img_size = img_n.size[0]\n",
    "    img_t = pil_to_tensor(img_n)\n",
    "    img_t = img_t.to(device)\n",
    "    def resize2d(img, size):\n",
    "        return (F.adaptive_avg_pool2d(Variable(img), size)).data\n",
    "    def upsample2d(img, size=224):\n",
    "        upsample = nn.Upsample(size=size, mode='bilinear', align_corners=False)\n",
    "        return upsample(torch.unsqueeze(img, 0))[0]\n",
    "else:\n",
    "    model_img_size = 224\n",
    "    img_n = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical exposure is in form 1/n s. Available: 15, 20, 25 30, 40, 50, 60, 80, 100, 125, 160, 200, 250\n",
    "exposure = 125 \n",
    "exp_micros = 1000000/exposure          # get exposure in microseconds\n",
    "img_ratio = 3024 / model_img_size      # every row in model is img_ratio rows in original image\n",
    "model_tr = 10 * img_ratio              # multiply real tr (10 micros) by img_ratio to find model tr\n",
    "conv_size = exp_micros / model_tr      # divide exposure time by tr to find convolution size\n",
    "conv_size = int(conv_size)             # Need closest integer approximation. Won't cause a significant difference\n",
    "conv_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(img, classid):\n",
    "    img_input = model_transform(img)\n",
    "    img_input = img_input.to(device)\n",
    "    pred = pretrained_model(img_input.unsqueeze(0))\n",
    "\n",
    "    #Use softmax to get predicted probability and view it\n",
    "    lay2 = torch.nn.Softmax(dim=1)\n",
    "    prob = lay2(pred)\n",
    "    maxOcls = prob.max(1)\n",
    "    return class_dict[maxOcls.indices.item()], round(maxOcls.values.item()*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_size + conv_size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdvPrediction(img, classid):\n",
    "    img_input = model_transform(img)\n",
    "    img_input = img_input.to(device)\n",
    "    batch_size = 4\n",
    "    w = torch.load('saved_inputs/imagenet_504_targeted585_color_resize252.pt', map_location=torch.device('cpu'))\n",
    "    #w = torch.ones(w.size())\n",
    "    w = w.to(device)\n",
    "    w = w[0]\n",
    "    # For resize post convolution\n",
    "    if account_resize:\n",
    "        w = torch.repeat_interleave(w, repeats=repeat_size, dim=1)\n",
    "    out_prediction = []\n",
    "    masked_imgs = []\n",
    "    out_imgs = []\n",
    "    out_prob = np.array([[0]*1000])\n",
    "    for j in tqdm(range(0,w.shape[1],batch_size)):\n",
    "        batch_limits = [j, min(w.shape[1],j+batch_size)]\n",
    "        sig_height = model_img_size + conv_size - 1\n",
    "        gy, c = applyMask(w, batch_limits, None, [0,0], sig_height, conv_size, 0)\n",
    "        \n",
    "        gy = torch.pow(gy, 1/2.2)\n",
    "        \n",
    "        #color correction\n",
    "        gy = torch.cat([imcorr(i).unsqueeze(0) for i in gy])\n",
    "        for i in range(0,len(image_list),batch_size):\n",
    "            if account_resize:\n",
    "                #For resize post convolution\n",
    "                inp = gy*img_t\n",
    "                masked_imgs+=[x.detach().cpu() for x in inp]\n",
    "                #Gaussian Noise\n",
    "                #inp = inp + torch.randn(inp.size(),device=device)*random.random()*0.05\n",
    "                #inp = torch.clamp(inp,min=0,max=1)\n",
    "                \n",
    "                #For multiple train images\n",
    "                #inp = gy*get_image_batch(gy.size()[0], 3024)\n",
    "                #inp = gy*get_image_batch(gy.size()[0], [k for k in range(i,i+batch_size)])\n",
    "    \n",
    "                inp = torch.cat([forward_normalize(upsample2d(i,224)).unsqueeze(0) for i in inp])\n",
    "            else: \n",
    "                inp = gy*inv_normalize(img_input)\n",
    "                inp = torch.cat([forward_normalize(i).unsqueeze(0) for i in inp])\n",
    "            \n",
    "            out = pretrained_model(inp)\n",
    "            prob = lay2(out)\n",
    "            #imshow_tensor(inp[1].detach().cpu(),inv_normalize)\n",
    "            tran = transforms.ToPILImage()\n",
    "            maxcls = prob.max(1)\n",
    "            #print(out.topk(2).indices[0])\n",
    "            out_prob = np.concatenate((out_prob,prob.detach().cpu().numpy()))\n",
    "            for k in range(batch_limits[1]-batch_limits[0]):\n",
    "                #print(\"alpha: {}\".format(c[i].item()))\n",
    "                c_value = round(c[k][0][0][0].data.item(),4)\n",
    "                out_imgs.append(tran(inv_normalize(inp[k].detach().cpu())))\n",
    "                out_prediction.append((class_dict[maxcls.indices[k].item()], round(maxcls.values[k].item()*100,4),class_dict[classid],round(prob[k][classid].data.item()*100,4),c_value))\n",
    "        del out\n",
    "        del inp\n",
    "        del gy\n",
    "        del prob\n",
    "        del maxcls\n",
    "    return out_prediction, out_imgs, out_prob, masked_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_prediction, out_img, out_prob, masked_imgs = getAdvPrediction(img_n, 504)\n",
    "#while(len([o for o in out_prediction if o[0]!=\"street sign\"]) < 110):\n",
    "#    out_prediction, out_img = getAdvPrediction(img, 919)\n",
    "#    print(len([o for o in out_prediction if o[0]!=\"street sign\"]))\n",
    "#out_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prob = out_prob.mean(axis=0)\n",
    "[(class_dict[x],x,mean_prob[x]) for x in np.argsort(mean_prob)[::-1]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cl = 504\n",
    "target_cl = 585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dodge_success\",(sum([1 for o in out_prediction if o[0]!=class_dict[source_cl]])*100)/len(out_prediction))\n",
    "print(\"target_success\",(sum([1 for o in out_prediction if o[0]==class_dict[target_cl]])*100)/len(out_prediction))\n",
    "vals = np.array([o[1] for o in out_prediction if o[0]==class_dict[target_cl]])\n",
    "print(\"target_confidence\", vals.mean())\n",
    "vals = np.array([o[1] for o in out_prediction if o[0]==class_dict[source_cl]])\n",
    "print(\"source_confidence\", vals.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = Image.open('images/original/test2.jpg')\n",
    "test2 = test2.resize((252,252))\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "min_val, min_img = 1000000,None\n",
    "for o in masked_imgs:\n",
    "    #print(imagehash.average_hash(test2)-imagehash.average_hash(tensor_to_pil(o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = tensor_to_pil(masked_imgs[4])\n",
    "phy = test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(0,200,250)\n",
    "coeff = np.polyfit(sim_vector,phy_vector,3,full=True)\n",
    "print(coeff)\n",
    "p = np.poly1d(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vector = np.array(sim)[:,:,0].flatten()\n",
    "phy_vector = np.array(phy)[:,:,0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(18,5))\n",
    "ax1.scatter(sim_vector, phy_vector, color='r')\n",
    "ax1.scatter([i for i in range(250)],[i for i in range(250)], color='k')\n",
    "ax1.scatter(x_test, p(x_test),color='b')\n",
    "ax2.scatter(np.array(sim)[:,:,1].flatten(), np.array(phy)[:,:,1].flatten(), color='g')\n",
    "ax2.scatter([i for i in range(250)],[i for i in range(250)], color='k')\n",
    "ax3.scatter(np.array(sim)[:,:,2].flatten(), np.array(phy)[:,:,2].flatten(), phy_vector, color='b')\n",
    "ax3.scatter([i for i in range(250)],[i for i in range(250)], color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageChops\n",
    "ImageChops.difference(test2, tensor_to_pil(masked_imgs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_resize[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_resize[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = np.array([out[0] for out in out_prediction])\n",
    "np.unique(lab,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ann_tm = annonate(border(out_img[0]), out_prediction[0])\n",
    "display(img_ann_tm)\n",
    "img_ann_tm.save(\"animation_images/919_0.5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToGif(filenames,fpss):\n",
    "    with imageio.get_writer('animation_images/movie.gif', mode='I',fps=fpss) as writer:\n",
    "        for image in filenames:\n",
    "            #image = imageio.imread(filename)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border(old_im):\n",
    "    #old_im = Image.open('c0.7_sz46_tg861.png')\n",
    "    old_size = old_im.size\n",
    "    new_size = (230, 260)\n",
    "    new_im = Image.new(\"RGB\", new_size,\"#FFF\")   ## luckily, this is already black!\n",
    "    new_im.paste(old_im, (3,0))\n",
    "\n",
    "    #display(new_im)\n",
    "    return new_im\n",
    "# new_im.save('someimage.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annonate(img, out):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # font = ImageFont.truetype(<font-file>, <font-size>)\n",
    "    #font = ImageFont.load_default()\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 10, encoding=\"unic\")\n",
    "    # draw.text((x, y),\"Sample Text\",(r,g,b))\n",
    "    draw.text((0, 224),\"Ambient Light = \"+str(out[4]),(0,0,0),font=font)\n",
    "    draw.text((0, 235),out[0][:15]+\" = \"+str(out[1]),(0,0,0),font=font)\n",
    "    draw.text((0, 246),out[2]+\" = \"+str(out[3]),(0,0,0),font=font)\n",
    "    #display(img)\n",
    "    return img\n",
    "#img.save('sample-out.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annonated_imgs = []\n",
    "for i in range(0,len(out_img),5):\n",
    "    im = out_img[i]\n",
    "    #if out_prediction[i][0]==\"street sign\": continue\n",
    "    img_ann = annonate(border(im), out_prediction[i])\n",
    "    #filenames.append(\"animation_images/shift_\"+str(i)+\".jpg\")\n",
    "    annonated_imgs.append(np.array(img_ann))\n",
    "    #img.save(\"animation_images/shift\"+str(i)+\".jpg\")\n",
    "writeToGif(annonated_imgs, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('renv': venv)",
   "language": "python",
   "name": "python36964bitrenvvenv63ce17223ffb457ca2f50b70abb95e80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
