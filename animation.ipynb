{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classes\n",
    "CLASS_URL = 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "class_dict = pickle.load(urllib.request.urlopen(CLASS_URL))\n",
    "\n",
    "#Retrieve image and assign class\n",
    "IMG_URL = \"https://bloximages.chicago2.vip.townnews.com/chippewa.com/content/tncms/assets/v3/editorial/c/ca/cca0d87d-8d7c-520f-b012-bc065d18b3dd/553cbf5aa0ffb.image.jpg?crop=200%2C200%2C0%2C8&resize=1200%2C1200&order=crop%2Cresize\"\n",
    "#IMG_URL = \"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSa3p29cIlgj0yQwMuLZzh8PKkgBUzbPLWrU-7K79DjDL498JsA&usqp=CAU\"\n",
    "#IMG_URL = \"https://thumbs-prod.si-cdn.com/D9GA8QHJQ70nFNQB-AFZ41S-Bs0=/fit-in/1600x0/https://public-media.si-cdn.com/filer/ea/21/ea2159df-9eec-4b05-a3de-356c57e23227/another_airplane_4676723312.jpg\"\n",
    "#IMG_URL = \"https://5.imimg.com/data5/OX/QV/SF/SELLER-23061093/national-35-model-air-rifle-500x500.jpg\"\n",
    "classidx = 919\n",
    "#classidx = 36\n",
    "response = requests.get(IMG_URL)\n",
    "img = Image.open(io.BytesIO(response.content))\n",
    "print(img.size)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "forward_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(type(img))\n",
    "img_input = model_transform(img)\n",
    "#img_input = img_input.unsqueeze(0)\n",
    "#img_input = Variable(img_input)\n",
    "img_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = models.resnet101(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "#Pass input through model\n",
    "img_input = img_input.to(device)\n",
    "pred = pretrained_model(img_input.unsqueeze(0))\n",
    "\n",
    "#Use softmax to get predicted probability and view it\n",
    "lay2 = torch.nn.Softmax(dim=1)\n",
    "prob = lay2(pred)\n",
    "maxOcls = prob.max(1)\n",
    "print(\"Class is {} ({}) with confidence {}%\".format(maxOcls.indices.item(),class_dict[maxOcls.indices.item()],maxOcls.values.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_mask(input_image):\n",
    "    #print(test[-3][:])\n",
    "    test = inv_normalize(input_image)\n",
    "    test = np.uint8(test.numpy()*255).transpose((1,2,0))\n",
    "    test = 1 - test\n",
    "    #test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(test, cv2.COLOR_RGB2GRAY)\n",
    "    th, thgray = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY_INV); \n",
    "    #blurred = cv2.GaussianBlur(thgray, (9, 9), 0)\n",
    "    th, blurred = cv2.threshold(thgray, 70, 255, cv2.THRESH_BINARY)\n",
    "    edged = cv2.Canny(blurred, 1, 250, L2gradient=True)   \n",
    "    #imgplot = plt.imshow(blurred, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    #imshow(input_image[0])\n",
    "    #applying closing function\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    closed = cv2.morphologyEx(blurred, cv2.MORPH_CLOSE, kernel)    #finding_contours\n",
    "    (cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)   \n",
    "    #cnts = [] #only for plate class \n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        cv2.drawContours(closed, [approx], -1, (0, 255, 0), 2)    \n",
    "    th, im_th = cv2.threshold(closed, 254, 255, cv2.THRESH_BINARY_INV);    \n",
    "    h, w = im_th.shape[:2]\n",
    "    im_floodfill = im_th.copy()\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 0);    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)    # Combine the two images to get the foreground.\n",
    "    #imshow(im_th)\n",
    "    #plt.show()\n",
    "    #imshow(im_floodfill_inv)\n",
    "    #plt.show()\n",
    "    im_out = im_th ^ im_floodfill_inv\n",
    "    im_out = cv2.GaussianBlur(im_out, (3, 3), 0)\n",
    "    return im_out\n",
    "print(type(img_input))\n",
    "imshow_tensor(img_input.cpu(),inv_normalize)\n",
    "#imm = Image.fromarray(get_object_mask(img_input.cpu()))\n",
    "#imm.save(\"class919_mask.png\")\n",
    "imgplot = plt.imshow(get_object_mask(img_input.cpu()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor(get_object_mask(img_input.cpu()), dtype=torch.float, device=device)\n",
    "mask = mask / torch.max(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMask(w, batch_limits, mask, c_limits):\n",
    "    \n",
    "    sz = w.shape[1]\n",
    "    \n",
    "    #stack the signal to fit the input size\n",
    "    oot = stack(w,228)             \n",
    "    batch = batch_limits[1]-batch_limits[0]\n",
    "    # EOT sampling for ambient light and shift\n",
    "    c = torch.rand([batch,1,1,1], device=device) * (c_limits[1] - c_limits[0]) + c_limits[0]\n",
    "    shift = torch.tensor(range(batch_limits[0],batch_limits[1]), dtype=torch.int)\n",
    "    #shift = torch.from_numpy(np.array(range(0,batch,1)))\n",
    "    #Shift the signal\n",
    "    ootn = shift_operation(oot.unsqueeze(0).repeat(batch,1,1,1).view(-1, 228, 1), shift).view(batch,3,228,1)\n",
    "    #Fit w into the range [0,1]. new_w is the same as ft\n",
    "    new_w = .5 * (torch.tanh(ootn) + 1)\n",
    "    \n",
    "    #Convolution of ft and the shutter\n",
    "    gy = lay(new_w.transpose(0,3).transpose(0,1)).transpose(0,1).transpose(0,3)\n",
    "    #Mask the signal to only affect the object\n",
    "    gy_mask = gy * mask\n",
    "    #return 0.5*c, new_w, c\n",
    "    return (c+ (1-c)*gy_mask), new_w, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(img, classid):\n",
    "    img_input = model_transform(img)\n",
    "    img_input = img_input.to(device)\n",
    "    pred = pretrained_model(img_input.unsqueeze(0))\n",
    "\n",
    "    #Use softmax to get predicted probability and view it\n",
    "    lay2 = torch.nn.Softmax(dim=1)\n",
    "    prob = lay2(pred)\n",
    "    maxOcls = prob.max(1)\n",
    "    return class_dict[maxOcls.indices.item()], round(maxOcls.values.item()*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdvPrediction(img, classid):\n",
    "    img_input = model_transform(img)\n",
    "    img_input = img_input.to(device)\n",
    "    w = torch.load('w_3_channel.pt')\n",
    "    w = w.to(device)\n",
    "    out_prediction = []\n",
    "    out_imgs = []\n",
    "    for j in range(0,w.shape[1],64):\n",
    "        batch_limits = [j, min(w.shape[1],j+64)]\n",
    "\n",
    "        gy, signal, c = applyMask(w,batch_limits, mask, [0.4,0.7])\n",
    "        inp = gy*inv_normalize(img_input)   \n",
    "        inp = torch.cat([forward_normalize(i).unsqueeze(0) for i in inp])\n",
    "        out = pretrained_model(inp)\n",
    "        prob = lay2(out)\n",
    "        #imshow_tensor(inp[1].detach().cpu(),inv_normalize)\n",
    "        tran = transforms.ToPILImage()\n",
    "        maxcls = prob.max(1)\n",
    "        #print(out.topk(2).indices[0])\n",
    "        \n",
    "        for k in range(batch_limits[1]-batch_limits[0]):\n",
    "            #print(\"alpha: {}\".format(c[i].item()))\n",
    "            c_value = round(c[k][0][0][0].data.item(),4)\n",
    "            out_imgs.append(tran(inv_normalize(inp[k].detach().cpu())))\n",
    "            out_prediction.append((class_dict[maxcls.indices[k].item()], round(maxcls.values[k].item()*100,4),class_dict[classid],round(prob[k][classid].data.item()*100,4),c_value))\n",
    "        del out\n",
    "        del inp\n",
    "        del signal\n",
    "        del gy\n",
    "        del prob\n",
    "        del maxcls\n",
    "    return out_prediction, out_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_prediction, out_img = getAdvPrediction(img, 919)\n",
    "#while(len([o for o in out_prediction if o[0]!=\"street sign\"]) < 110):\n",
    "#    out_prediction, out_img = getAdvPrediction(img, 919)\n",
    "#    print(len([o for o in out_prediction if o[0]!=\"street sign\"]))\n",
    "out_prediction\n",
    "vals = np.array([o[3] for o in out_prediction if o[2]==class_dict[919]])\n",
    "vals.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ann_tm = annonate(border(out_img[0]), out_prediction[0])\n",
    "display(img_ann_tm)\n",
    "img_ann_tm.save(\"animation_images/919_0.5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToGif(filenames,fpss):\n",
    "    with imageio.get_writer('animation_images/movie.gif', mode='I',fps=fpss) as writer:\n",
    "        for image in filenames:\n",
    "            #image = imageio.imread(filename)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border(old_im):\n",
    "    #old_im = Image.open('c0.7_sz46_tg861.png')\n",
    "    old_size = old_im.size\n",
    "    new_size = (230, 260)\n",
    "    new_im = Image.new(\"RGB\", new_size,\"#FFF\")   ## luckily, this is already black!\n",
    "    new_im.paste(old_im, (3,0))\n",
    "\n",
    "    #display(new_im)\n",
    "    return new_im\n",
    "# new_im.save('someimage.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annonate(img, out):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # font = ImageFont.truetype(<font-file>, <font-size>)\n",
    "    #font = ImageFont.load_default()\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 10, encoding=\"unic\")\n",
    "    # draw.text((x, y),\"Sample Text\",(r,g,b))\n",
    "    draw.text((0, 224),\"Ambient Light = \"+str(out[4]),(0,0,0),font=font)\n",
    "    draw.text((0, 235),out[0][:15]+\" = \"+str(out[1]),(0,0,0),font=font)\n",
    "    draw.text((0, 246),out[2]+\" = \"+str(out[3]),(0,0,0),font=font)\n",
    "    #display(img)\n",
    "    return img\n",
    "#img.save('sample-out.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annonated_imgs = []\n",
    "for i in range(0,len(out_img),5):\n",
    "    im = out_img[i]\n",
    "    #if out_prediction[i][0]==\"street sign\": continue\n",
    "    img_ann = annonate(border(im), out_prediction[i])\n",
    "    #filenames.append(\"animation_images/shift_\"+str(i)+\".jpg\")\n",
    "    annonated_imgs.append(np.array(img_ann))\n",
    "    #img.save(\"animation_images/shift\"+str(i)+\".jpg\")\n",
    "writeToGif(annonated_imgs, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('renv': venv)",
   "language": "python",
   "name": "python36964bitrenvvenv63ce17223ffb457ca2f50b70abb95e80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
