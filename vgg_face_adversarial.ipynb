{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "from utils import *\n",
    "from resnet50_ft_dims_2048 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[91.4953, 103.8827, 131.0912],\n",
    "                                                              std=[1, 1, 1])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50_Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(2048, 500)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # Get the flattened vector from the backbone of resnet50\n",
    "        return self.fc(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_class = ResNet50_Classifier()\n",
    "state_dict = torch.load(\"vgg2_classifier_500.pt\")\n",
    "model_class.load_state_dict(state_dict)\n",
    "\n",
    "model = resnet50_ft(\"vgg_face_testimages/resnet50_ft_dims_2048.pth\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_class.to(device)\n",
    "model_class.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pixel = torch.DoubleTensor([131.0912, 103.8827, 91.4953])\n",
    "def bounding_crop(img, bounding_box):\n",
    "    im_shape = np.array(img.size)\n",
    "    x,y,w,h = bounding_box\n",
    "    half_extension = 0.15\n",
    "    area = (max(0,x-half_extension*w), max(0,y-half_extension*h),\n",
    "            min(im_shape[0], x+w*(1+half_extension*2)), min(im_shape[1], y+h*(1+half_extension*2)))\n",
    "    img = img.crop(area)\n",
    "    return img\n",
    "\n",
    "def load_data(img, shape=None, bounding_box=None):\n",
    "        # in the format of (width, height, *)\n",
    "    im_shape = np.array(img.size)\n",
    "\n",
    "    short_size = 224.0\n",
    "    crop_size = shape\n",
    "    img = img.convert('RGB')\n",
    "    \n",
    "    ratio = float(short_size) / np.min(im_shape)\n",
    "    img = img.resize(size=(int(np.ceil(im_shape[0] * ratio)),   # width\n",
    "                           int(np.ceil(im_shape[1] * ratio))),  # height\n",
    "                     resample=PIL.Image.BILINEAR)\n",
    "\n",
    "    x = np.array(img)  # image has been transposed into (height, width)\n",
    "    newshape = x.shape[:2]\n",
    "    h_start = (newshape[0] - crop_size[0])//2\n",
    "    w_start = (newshape[1] - crop_size[1])//2\n",
    "    x = x[h_start:h_start+crop_size[0], w_start:w_start+crop_size[1]]\n",
    "    return x-mean\n",
    "def fetch_images(paths):\n",
    "    images = []\n",
    "    for im in paths:\n",
    "        images.append(Image.open(im))\n",
    "    return images\n",
    "def prepare_input(img_list):\n",
    "    im_array = np.array([load_data(img=x, shape=(224, 224, 3)) for x in img_list])\n",
    "    im_tensor = torch.Tensor(im_array.transpose(0, 3, 1, 2))\n",
    "    return im_tensor\n",
    "\n",
    "def prepare_input2(img_list):\n",
    "    t_list = []\n",
    "    for y in img_list:\n",
    "        #y = model_transform(y)\n",
    "        y[0] = y[0] - mean_pixel[0]/255\n",
    "        y[1] = y[1] - mean_pixel[1]/255\n",
    "        y[2] = y[2] - mean_pixel[2]/255\n",
    "        t_list.append(y*255)\n",
    "    return torch.stack(t_list)\n",
    "\n",
    "def prepare_input_adv(img_list):\n",
    "    t_list = []\n",
    "    for y in img_list:\n",
    "        y[0] = y[0] - mean_pixel[0]/255\n",
    "        y[1] = y[1] - mean_pixel[1]/255\n",
    "        y[2] = y[2] - mean_pixel[2]/255\n",
    "        t_list.append(y*255)\n",
    "    return torch.stack(t_list)\n",
    "\n",
    "def prepare_input_inv(img_list):\n",
    "    t_list = []\n",
    "    for y in img_list:\n",
    "        y[0] = y[0]/255 + mean_pixel[0]/255\n",
    "        y[1] = y[1]/255 + mean_pixel[1]/255\n",
    "        y[2] = y[2]/255 + mean_pixel[2]/255\n",
    "        t_list.append(y)\n",
    "    return torch.stack(t_list)\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "\n",
    "    acc = acc * 100\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def get_prediction(y_pred, classid=None):\n",
    "    lay2 = torch.nn.Softmax(dim=1)\n",
    "    y_pred_softmax = lay2(y_pred)\n",
    "    if classid:\n",
    "        conf, y_pred_tags = torch.mean(y_pred_softmax[:,classid]), classid\n",
    "    #else:\n",
    "        #conf, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "    return y_pred_tags, conf\n",
    "'''\n",
    "im_array = np.array([load_data(img=i, shape=(224, 224, 3), bounding_box = bounding_dict[image_path_list[ind].split(\"test/\")[1].split(\".\")[0]]) for ind, i in enumerate(images)])\n",
    "im_tensor = torch.Tensor(im_array.transpose(0, 3, 1, 2))\n",
    "labels = [self.class_dict[int(path.split(\"/\")[-2].split(\"n00\")[-1])] for path in image_path_list]\n",
    "\n",
    "labels = torch.tensor(labels,dtype=torch.long)\n",
    "labels = labels.squeeze(0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "vgg_bounding = \"/nobackup/vgg2face/bb_landmark/\"\n",
    "bounding_dict = {}\n",
    "\n",
    "with open(vgg_bounding+'loose_bb_test.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            bounding_dict[row[0]] = [int(row[1]), int(row[2]), int(row[3]), int(row[4])]\n",
    "            line_count += 1\n",
    "            \n",
    "class_dict = pickle.load(open(\"vgg2_testset_classdict.pk\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in class_dict.items():\n",
    "    if v == 272:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = [\"n004064/\"+x for x in os.listdir(test_dir+\"n004064\")[:35]]\n",
    "src_img = fetch_images([test_dir+x for x in img_file])\n",
    "src_img = [bounding_crop(x, bounding_dict[y.split(\".\")[0]]) for x,y in zip(src_img,img_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/nobackup/vgg2face/test/\"\n",
    "img_file = \"n004891/\"+os.listdir(test_dir+\"n004891\")[29]\n",
    "tar_file = \"n005148/\"+os.listdir(test_dir+\"n005148\")[8]\n",
    "src_img = fetch_images([test_dir+img_file])[0]\n",
    "trg_exm = fetch_images([test_dir+tar_file])[0]\n",
    "src_img = bounding_crop(src_img, bounding_dict[img_file.split(\".\")[0]])\n",
    "display(src_img)\n",
    "trg_exm = bounding_crop(trg_exm, bounding_dict[tar_file.split(\".\")[0]])\n",
    "display(trg_exm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean = (131.0912, 103.8827, 91.4953)\n",
    "model_input = model_transform(src_img[3])\n",
    "model_input = prepare_input2([model_input])\n",
    "\n",
    "print(display(transToPIL(model_input[0])))\n",
    "model_input = model_input.to(device)\n",
    "\n",
    "y_pred = model(model_input)[1]\n",
    "\n",
    "y_pred = y_pred.squeeze(-1)\n",
    "y_pred = y_pred.squeeze(-1)\n",
    "\n",
    "y_pred = model_class(y_pred)\n",
    "print(get_prediction(y_pred))\n",
    "print(class_dict[3009])\n",
    "print(class_dict[5427])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transToPIL = transforms.ToPILImage()\n",
    "transToTensor = transforms.ToTensor()\n",
    "model_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.RandomCrop((224,224)),\n",
    "                                         transforms.ToTensor()])\n",
    "reverse_normalize = transforms.Normalize(mean=[-131.0912, -103.8827, -91.4953],\n",
    "                                                              std=[1,1,1])\n",
    "forward_normalize = transforms.Normalize(mean=[131.0912, 103.8827, 91.4953],\n",
    "                                                              std=[1,1,1])\n",
    "#display(src_img)\n",
    "#x = np.array(src_img)\n",
    "#x = x - mean\n",
    "#x = x.transpose(2,0,1)\n",
    "#xx = torch.Tensor(x)\n",
    "#display(transToPIL(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (224,224)\n",
    "mask = torch.ones(input_size,dtype=torch.float, device=device)\n",
    "targidx = 363\n",
    "classidx = 209\n",
    "im_height = 224\n",
    "print(im_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 120   #Length of input signal\n",
    "c = .1     #Ambient light ratio\n",
    "c_limits = [0.3,0.7]\n",
    "batch = 16\n",
    "channels = 1\n",
    "# change of variable term to optimise on\n",
    "w = torch.rand([channels,sz,1], requires_grad=True, dtype=torch.float, device=device)\n",
    "rescale_factor = 1\n",
    "#Create the mask to only illuminate the object\n",
    "#mask = torch.tensor(get_object_mask(img_input.cpu()), dtype=torch.float, device=device)\n",
    "#mask = mask / torch.max(mask)\n",
    "\n",
    "\n",
    "#Target and original class labels\n",
    "target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "orig = torch.tensor([classidx], dtype=torch.long, device=device)\n",
    "\n",
    "#Model parameters\n",
    "lr = 1e-1\n",
    "n_epochs = 5\n",
    "optimizer = optim.SGD([w], lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_img = Image.new('RGB', (800,1280), (255, 255, 255))\n",
    "src_img = [src_img]*25\n",
    "targidx = 272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track the loss to target and original class\n",
    "targloss = []\n",
    "origloss = []\n",
    "n_epochs = 3000\n",
    "#obj_dict = {}\n",
    "\n",
    "#Optimisation loop. initially untargeted\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    \n",
    "    #Switch to targeted at halfway point\n",
    "    half = False#epoch < n_epochs//6\n",
    "    if epoch == -1:#n_epochs//6:\n",
    "        tops = out.topk(2).indices[0]\n",
    "        targidx = tops[0].item() if tops[0].item() != classidx else tops[1].item()\n",
    "        target = torch.tensor([targidx], dtype=torch.long, device=device)\n",
    "        print(\"Switching from untarget to target {}\".format(targidx))\n",
    "    \n",
    "    if channels==1:\n",
    "        n_w = w.repeat(3,1,1)\n",
    "    else:\n",
    "        n_w = w\n",
    "    if rescale_factor!=1:\n",
    "        dim = len(torch.flatten(n_w))//3\n",
    "        n_dim = rescale_factor*dim\n",
    "        n_w = n_w.unsqueeze(0)\n",
    "        t = nn.Upsample(size=(n_dim,1), mode='bilinear')\n",
    "        n_w = t(n_w)[0]\n",
    "    \n",
    "    gy, new_w = fttogy(n_w, batch, mask, c_limits, im_height+4)\n",
    "    \n",
    "    model_input = model_transform(src_img[epoch%25])\n",
    "    input_img2 = model_input.to(device)\n",
    "    inp = gy*input_img2   \n",
    "    inp2 = prepare_input_adv(inp)\n",
    "    \n",
    "    out = model(inp2)[1]\n",
    "    out = out.squeeze(-1)\n",
    "    out = out.squeeze(-1)\n",
    "    out = model_class(out)\n",
    "    \n",
    "    #Calculate Loss depended on if targeted or untargeted\n",
    "    if not half: targLoss = loss_fn(out, target.repeat(batch))\n",
    "    origLoss = loss_fn(out, orig.repeat(batch))\n",
    "    loss = -origLoss if half else targLoss\n",
    "    if epoch%100 == 0:\n",
    "        targloss.append(0 if half else targLoss)\n",
    "        origloss.append(origLoss)\n",
    "        if not half: print(targLoss, origLoss) \n",
    "    loss.backward()   \n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    del loss\n",
    "    if epoch!=n_epochs-1:\n",
    "        del inp\n",
    "        del new_w\n",
    "    #else:\n",
    "        #saving w to be used for prediction\n",
    "        #torch.save(n_w,'w_0.5_764.pt')\n",
    "\n",
    "    #Code to check gpu allocation    \n",
    "    '''\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                #print(type(obj), obj.size())\n",
    "                if type(obj) not in obj_dict:\n",
    "                    obj_dict[type(obj)] = 1\n",
    "                else:\n",
    "                    obj_dict[type(obj)] += 1\n",
    "        except: pass\n",
    "    print(obj_dict)\n",
    "    obj_dict.clear()\n",
    "    '''\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "#View original loss and target loss\n",
    "plt.plot(targloss, label=\"target\")\n",
    "plt.plot(origloss, label=\"original\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(n_w, classidx, targidx, [src_img[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inp[0].size())\n",
    "print(display(transToPIL((gy.cpu()*model_transform(src_img[0]))[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMask(w, batch_limits, mask, c_limits):\n",
    "    \n",
    "    sz = w.shape[1]\n",
    "    \n",
    "    #stack the signal to fit the input size\n",
    "    oot = stack(w,228)             \n",
    "    batch = batch_limits[1]-batch_limits[0]\n",
    "    # EOT sampling for ambient light and shift\n",
    "    c = torch.rand([batch,1,1,1], device=device) * (c_limits[1] - c_limits[0]) + c_limits[0]\n",
    "    shift = torch.tensor(range(batch_limits[0],batch_limits[1]), dtype=torch.int)\n",
    "    #shift = torch.from_numpy(np.array(range(0,batch,1)))\n",
    "    #Shift the signal\n",
    "    ootn = shift_operation(oot.unsqueeze(0).repeat(batch,1,1,1).view(-1, 228, 1), shift).view(batch,3,228,1)\n",
    "    #Fit w into the range [0,1]. new_w is the same as ft\n",
    "    new_w = .5 * (torch.tanh(ootn) + 1)\n",
    "    \n",
    "    #Convolution of ft and the shutter\n",
    "    gy = lay(new_w.transpose(0,3).transpose(0,1)).transpose(0,1).transpose(0,3)\n",
    "    #Mask the signal to only affect the object\n",
    "    gy_mask = gy * mask\n",
    "    #return 0.5*c, new_w, c\n",
    "    return (c+ (1-c)*gy_mask), new_w, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = [x for x in os.listdir(\"face_rec_results/\") if x.startswith(\"n00\")]\n",
    "test_dir = \"/nobackup/vgg2face/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(n_ww, classidx, targidx, src_img):\n",
    "    n_ww = n_ww.to(device)\n",
    "    classid_over = []\n",
    "    targid_over = []\n",
    "    top_over = []\n",
    "    batch_size = 1\n",
    "    for s in range(0,len(src_img),batch_size):\n",
    "        #print(s)\n",
    "        src_img_batch = src_img[s:s+batch_size]\n",
    "        #sr = src_img[s]\n",
    "        #display(src_img[s])\n",
    "        model_input = [model_transform(x) for x in src_img_batch]\n",
    "        model_input = torch.stack(model_input)\n",
    "        #print(display(transToPIL(model_input)))\n",
    "        batch_limits = [0,120]\n",
    "        gy, signal, c = applyMask(n_ww,batch_limits, mask, [0.3,0.7])\n",
    "        #print(input_img2)\n",
    "\n",
    "        classid = []\n",
    "        targid = []\n",
    "        topid = []\n",
    "        for i in range(len(gy)):\n",
    "            #print(i)\n",
    "            input_img2 = model_input.to(device)\n",
    "            #print(input_img2.size())\n",
    "            multi = gy[i]*input_img2\n",
    "            #print(multi.size())\n",
    "            if i==0:\n",
    "                print(display(transToPIL(multi[0].cpu())))\n",
    "            multi2 = prepare_input_adv(multi)\n",
    "            #print(multi2.size())\n",
    "            #print(display(transToPIL(multi2[0].cpu())))\n",
    "            #print(multi2[0].cpu())\n",
    "            y_pred = model(multi2)[1]\n",
    "\n",
    "            y_pred = y_pred.squeeze(-1)\n",
    "            y_pred = y_pred.squeeze(-1)\n",
    "\n",
    "            y_pred = model_class(y_pred)\n",
    "\n",
    "            classid.append(get_prediction(y_pred, classidx)[1].cpu().detach().item())\n",
    "            targid.append(get_prediction(y_pred, targidx)[1].cpu().detach().item())\n",
    "            #topid.append(get_prediction(y_pred)[1].cpu().detach().item())\n",
    "            #print(get_prediction(y_pred, classidx))\n",
    "            #print(get_prediction(y_pred, targidx))\n",
    "        #print(\"source\", classidx, np.array(classid).mean())\n",
    "        #print(\"target\", targidx, np.array(targid).mean())\n",
    "        classid_over.append(np.array(classid).mean())\n",
    "        targid_over.append(np.array(targid).mean())\n",
    "    return np.array(classid_over), np.array(targid_over)\n",
    "#print(\"source\", classidx, np.array(classid_over).mean())\n",
    "#print(\"target\", targidx, np.array(targid_over).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for r in tqdm(range(len(result_list))):\n",
    "    res = result_list[r]\n",
    "    if res.split('_')[1]!='212': continue\n",
    "    n_ww = torch.load(\"face_rec_results/\"+res)\n",
    "\n",
    "    img_file = [res.split(\"_\")[0]+\"/\"+x for x in os.listdir(test_dir+res.split(\"_\")[0])]\n",
    "    src_img = fetch_images([test_dir+x for x in img_file[:1]])\n",
    "    src_img = [bounding_crop(x, bounding_dict[y.split(\".\")[0]]) for x,y in zip(src_img,img_file)]\n",
    "    \n",
    "    classidx = int(res.split(\"_\")[1])\n",
    "    targidx = int(res.split(\"_\")[2].split(\".\")[0])\n",
    "    \n",
    "    classid_over, targid_over = get_results(n_ww, classidx, targidx, src_img)\n",
    "    for co in classid_over:\n",
    "        results.append((classidx, 'source', co))\n",
    "    for co in targid_over:\n",
    "        results.append((classidx, 'target', co))\n",
    "    #print(\"source\", classidx, np.array(classid_over).mean())\n",
    "    #print(\"target\", targidx, np.array(targid_over).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "\n",
    "# load dataset\n",
    "df = pd.DataFrame(results, columns = ['Class_id', 'Type', 'Conf'])\n",
    "df['aggr_conf'] = df.groupby(['Class_id', 'Type'])['Conf'].transform('mean')\n",
    "classes = df.Class_id.unique()\n",
    "#df = df[df['Class_id'].isin(classes[160:])]\n",
    "filtered = df#df[(df['aggr_conf']<0.75)&(df['Type']=='target')|(df['aggr_conf']>0.05)&(df['Type']=='source')]\n",
    "sns.barplot(x = 'Class_id', y = 'Conf', hue = 'Type', data = filtered,\n",
    "            palette = 'Blues', edgecolor = 'w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby(['Type']).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_across_transformation(n_ww, classidx, targidx, src_img, shift_range, light_range):\n",
    "    n_ww = n_ww.to(device)\n",
    "    results = []\n",
    "    batch_size = 1\n",
    "    for s in range(0,len(src_img),batch_size):\n",
    "        result = {'shift':[],'light':[]}\n",
    "        src_img_batch = src_img[s:s+batch_size]\n",
    "        model_input = [model_transform(x) for x in src_img_batch]\n",
    "        model_input = torch.stack(model_input)\n",
    "        batch_limits = shift_range\n",
    "        gy, signal, c = applyMask(n_ww,batch_limits, mask, [float(light_range[0])/10,float(light_range[1])/10])\n",
    "        \n",
    "        for i in range(len(gy)):\n",
    "            input_img2 = model_input.to(device)\n",
    "            multi = gy[i]*input_img2\n",
    "            multi2 = prepare_input_adv(multi)\n",
    "            y_pred = model(multi2)[1]\n",
    "\n",
    "            y_pred = y_pred.squeeze(-1)\n",
    "            y_pred = y_pred.squeeze(-1)\n",
    "\n",
    "            y_pred = model_class(y_pred)\n",
    "\n",
    "            class_acc = get_prediction(y_pred, classidx)[1].cpu().detach().item()\n",
    "            targ_acc = get_prediction(y_pred, targidx)[1].cpu().detach().item()\n",
    "            \n",
    "            result['shift'].append((class_acc,targ_acc))\n",
    "        \n",
    "        \n",
    "        for i in range(light_range[0],light_range[1]+1):\n",
    "            gy, signal, c = applyMask(n_ww,batch_limits, mask, [float(i)/10,float(i)/10])\n",
    "            class_ac = []\n",
    "            targ_ac = []\n",
    "            for j in range(len(gy)):\n",
    "                input_img2 = model_input.to(device)\n",
    "                multi = gy[j]*input_img2\n",
    "                multi2 = prepare_input_adv(multi)\n",
    "                y_pred = model(multi2)[1]\n",
    "\n",
    "                y_pred = y_pred.squeeze(-1)\n",
    "                y_pred = y_pred.squeeze(-1)\n",
    "\n",
    "                y_pred = model_class(y_pred)\n",
    "\n",
    "                class_acc = get_prediction(y_pred, classidx)[1].cpu().detach().item()\n",
    "                targ_acc = get_prediction(y_pred, targidx)[1].cpu().detach().item()\n",
    "\n",
    "                class_ac.append(class_acc)\n",
    "                targ_ac.append(targ_acc)\n",
    "            result['light'].append((np.array(class_ac).mean(), np.array(targ_ac).mean()))\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_shift = []\n",
    "results_light = []\n",
    "for r in tqdm(range(len(result_list[30:50]))):\n",
    "    res = result_list[r]\n",
    "    #if res.split('_')[1]!='211': continue\n",
    "    n_ww = torch.load(\"face_rec_results/\"+res)\n",
    "\n",
    "    img_file = [res.split(\"_\")[0]+\"/\"+x for x in os.listdir(test_dir+res.split(\"_\")[0])]\n",
    "    src_img = fetch_images([test_dir+x for x in img_file[30:31]])\n",
    "    src_img = [bounding_crop(x, bounding_dict[y.split(\".\")[0]]) for x,y in zip(src_img,img_file)]\n",
    "    \n",
    "    classidx = int(res.split(\"_\")[1])\n",
    "    targidx = int(res.split(\"_\")[2].split(\".\")[0])\n",
    "    \n",
    "    result = get_results_across_transformation(n_ww, classidx, targidx, src_img, [0,120], [3,7])\n",
    "    for j in range(len(result)):\n",
    "        for i,res in enumerate(result[j]['shift']):\n",
    "            results_shift.append((str(classidx)+str(j), 'source', i, res[0]))\n",
    "            results_shift.append((str(classidx)+str(j), 'target', i, res[1]))\n",
    "    for j in range(len(result)):\n",
    "        for i,res in enumerate(result[j]['light']):\n",
    "            results_light.append((str(classidx)+str(j), 'source', 3+i, res[0]))\n",
    "            results_light.append((str(classidx)+str(j), 'target', 3+i, res[1]))\n",
    "    #print(\"source\", classidx, np.array(classid_over).mean())\n",
    "    #print(\"target\", targidx, np.array(targid_over).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "\n",
    "# load dataset\n",
    "df = pd.DataFrame(results_shift, columns = ['Class_id', 'Type', 'shift','Conf'])\n",
    "sns.barplot(x = 'Class_id', y = 'Conf', hue = 'Type', data = df,\n",
    "            palette = 'Blues', edgecolor = 'w')\n",
    "plt.show()\n",
    "\n",
    "df = pd.DataFrame(results_light, columns = ['Class_id', 'Type', 'light','Conf'])\n",
    "sns.barplot(x = 'Class_id', y = 'Conf', hue = 'Type', data = df,\n",
    "            palette = 'Blues', edgecolor = 'w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_done=os.listdir(\"face_rec_results/\")\n",
    "files_all=os.listdir(\"/nobackup/vgg2face/test/\")\n",
    "files_left = [x for x in files_all if x not in [x.split(\"_\")[0] for x in files_done]]\n",
    "random.shuffle(files_left)\n",
    "print(len(files_left))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"script_gpu_2.sh\",\"w\") as ff:\n",
    "    for f in files_left[:30]:\n",
    "        ff.write(\"python face_recog_generate.py \"+f+\" 2\\n\")\n",
    "    ff.close()\n",
    "with open(\"script_gpu_1.sh\",\"w\") as ff:\n",
    "    for f in files_left[30:60]:\n",
    "        ff.write(\"python face_recog_generate.py \"+f+\" 1\\n\")\n",
    "    ff.close()\n",
    "with open(\"script_gpu_3.sh\",\"w\") as ff:\n",
    "    for f in files_left[60:90]:\n",
    "        ff.write(\"python face_recog_generate.py \"+f+\" 3\\n\")\n",
    "    ff.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('renv': venv)",
   "language": "python",
   "name": "python36964bitrenvvenv63ce17223ffb457ca2f50b70abb95e80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
