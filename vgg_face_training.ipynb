{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import urllib.request\n",
    "import requests\n",
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "from utils import *\n",
    "from resnet50_ft_dims_2048 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display image and predicted class\n",
    "def imstats(name):\n",
    "    # read in image and view it\n",
    "    x = readim(name, forward_normalize) \n",
    "    imshow_tensor(x[0], inv_normalize)\n",
    "\n",
    "    #get predicted class and probability\n",
    "    prob = lay2(pretrained_model(x.cuda()))\n",
    "    maxcls = prob.max(1)\n",
    "    print(\"Class is {} ({}) with confidence {}%\".format(maxcls.indices.item(),class_dict[maxcls.indices.item()],round(maxcls.values.item()*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_size = 224\n",
    "model_transform = transforms.Compose([transforms.Resize((model_img_size,model_img_size)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[91.4953, 103.8827, 131.0912],\n",
    "                                                              std=[1, 1, 1])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_FEATURES = 2048\n",
    "NUM_CLASSES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50_Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(2048, 500)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # Get the flattened vector from the backbone of resnet50\n",
    "        return self.fc(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_class = ResNet50_Classifier()\n",
    "\n",
    "model = resnet50_ft(\"vgg_face_testimages/resnet50_ft_dims_2048.pth\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_class.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class.to(device)\n",
    "model_class.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_crop(img, bounding_box):\n",
    "    im_shape = np.array(img.size)\n",
    "    x,y,w,h = bounding_box\n",
    "    half_extension = 0.15\n",
    "    area = (max(0,x-half_extension*w), max(0,y-half_extension*h),\n",
    "            min(im_shape[0], x+w*(1+half_extension*2)), min(im_shape[1], y+h*(1+half_extension*2)))\n",
    "    img = img.crop(area)\n",
    "    return img\n",
    "\n",
    "def load_data(img, shape=None, bounding_box=None):\n",
    "    im_shape = np.array(img.size)    # in the format of (width, height, *)\n",
    "    \n",
    "    if bounding_box:\n",
    "        x,y,w,h = bounding_box\n",
    "        half_extension = 0.15\n",
    "        area = (max(0,x-half_extension*w), max(0,y-half_extension*h),\n",
    "                min(im_shape[0], x+w*(1+half_extension*2)), min(im_shape[1], y+h*(1+half_extension*2)))\n",
    "        img = img.crop(area)\n",
    "    \n",
    "    #return model_transform(img)\n",
    "    im_shape = np.array(img.size)\n",
    "\n",
    "    short_size = 224.0\n",
    "    crop_size = shape\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    ratio = float(short_size) / np.min(im_shape)\n",
    "    img = img.resize(size=(int(np.ceil(im_shape[0] * ratio)),   # width\n",
    "                           int(np.ceil(im_shape[1] * ratio))),  # height\n",
    "                     resample=PIL.Image.BILINEAR)\n",
    "\n",
    "    x = np.array(img)  # image has been transposed into (height, width)\n",
    "    newshape = x.shape[:2]\n",
    "    h_start = (newshape[0] - crop_size[0])//2\n",
    "    w_start = (newshape[1] - crop_size[1])//2\n",
    "    x = x[h_start:h_start+crop_size[0], w_start:w_start+crop_size[1]]\n",
    "    x = x - mean\n",
    "    return x\n",
    "\n",
    "def image_encoding(model, images):\n",
    "    #print('==> compute image-level feature encoding.')\n",
    "    num_faces = len(images)\n",
    "    im_array = np.array([load_data(img=i, shape=(224, 224, 3)) for i in images])\n",
    "    im_tensor = torch.Tensor(im_array.transpose(0, 3, 1, 2))\n",
    "    im_tensor = im_tensor.to(device)\n",
    "    f  = model(im_tensor)\n",
    "    classif = f[0]\n",
    "    feat = f[1].detach().cpu().numpy()[:, :, 0, 0]\n",
    "    face_feats = feat / np.sqrt(np.sum(feat ** 2, -1, keepdims=True))\n",
    "    return classif, face_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_features(images):\n",
    "    out, face_feats = image_encoding(model, images)\n",
    "    return out, face_feats\n",
    "\n",
    "def fetch_images(paths):\n",
    "    images = []\n",
    "    for im in paths:\n",
    "        images.append(Image.open(im))\n",
    "    return images\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    svm_input = []\n",
    "    svm_labels = []\n",
    "    for i in tqdm(range(0,len(dataset),30)):\n",
    "        batch = dataset[i:min(i+30,len(dataset))]\n",
    "        images = fetch_images(batch)\n",
    "        out, feats = predict_features(images)\n",
    "        for ind, path in enumerate(batch):\n",
    "            svm_input.append(feats[ind])\n",
    "            svm_labels.append(int(path.split(\"/\")[-2].split(\"n00\")[-1]))\n",
    "        del images\n",
    "        del feats\n",
    "        del out\n",
    "    return svm_input, svm_labels\n",
    "\n",
    "class VGG2Dataset(Dataset):\n",
    "    def __init__(self, image_paths, class_dict, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.class_dict = class_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.image_paths)/BATCH_SIZE)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            idx = [idx]\n",
    "        idx = idx[0]\n",
    "        image_path_list = self.image_paths[idx*BATCH_SIZE:(idx+1)*BATCH_SIZE]\n",
    "        if len(image_path_list)!=BATCH_SIZE:\n",
    "            image_path_list += random.choices(self.image_paths, k=(BATCH_SIZE-len(image_path_list)))\n",
    "        images = fetch_images(image_path_list)\n",
    "        \n",
    "        num_faces = len(images)\n",
    "        #im_tensor = torch.stack([load_data(img=i, shape=(224, 224, 3), bounding_box = bounding_dict[image_path_list[ind].split(\"test/\")[1].split(\".\")[0]]) for ind, i in enumerate(images)])\n",
    "        im_array = np.array([load_data(img=i, shape=(224, 224, 3), bounding_box = bounding_dict[image_path_list[ind].split(\"test/\")[1].split(\".\")[0]]) for ind, i in enumerate(images)])\n",
    "        im_tensor = torch.Tensor(im_array.transpose(0, 3, 1, 2))\n",
    "        labels = [self.class_dict[int(path.split(\"/\")[-2].split(\"n00\")[-1])] for path in image_path_list]\n",
    "        \n",
    "        labels = torch.tensor(labels,dtype=torch.long)\n",
    "        labels = labels.squeeze(0)\n",
    "        return im_tensor, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_occuring_label(pred):\n",
    "    #Use softmax to get predicted probability and view it\n",
    "    lay2 = torch.nn.Softmax(dim=1)\n",
    "    prob = lay2(pred)\n",
    "    maxOcls = prob.max(1)\n",
    "    labels, counts = maxOcls.indices.unique(return_counts=True)\n",
    "    return labels[counts.max(0)[1]], counts[counts.max(0)[1]], counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "vgg_bounding = \"/nobackup/vgg2face/bb_landmark/\"\n",
    "bounding_dict = {}\n",
    "\n",
    "with open(vgg_bounding+'loose_bb_test.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            bounding_dict[row[0]] = [int(row[1]), int(row[2]), int(row[3]), int(row[4])]\n",
    "            line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (131.0912, 103.8827, 91.4953)\n",
    "model_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.RandomCrop((224,224)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[131.0912, 103.8827, 91.4953],\n",
    "                                                              std=[1,1,1])])\n",
    "batch_size = 1\n",
    "\n",
    "vgg_test_dir = \"/nobackup/vgg2face/test/\"\n",
    "class_dir_list = os.listdir(vgg_test_dir)\n",
    "train_dataset_paths = []\n",
    "test_dataset_paths = []\n",
    "\n",
    "for class_dir in class_dir_list:\n",
    "    class_dir = vgg_test_dir+class_dir+\"/\"\n",
    "    all_images = []\n",
    "    for image in os.listdir(class_dir):\n",
    "        all_images.append(class_dir+image)\n",
    "    random.shuffle(all_images)\n",
    "    test_dataset_paths += all_images[:50]\n",
    "    train_dataset_paths += all_images[50:]\n",
    "random.shuffle(test_dataset_paths)\n",
    "random.shuffle(train_dataset_paths)\n",
    "class_d = pickle.load(open(\"vgg2_testset_classdict.pk\",\"rb\"))\n",
    "train_dataset = VGG2Dataset(image_paths=train_dataset_paths, class_dict = class_d)\n",
    "test_dataset = VGG2Dataset(image_paths=test_dataset_paths, class_dict = class_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=1\n",
    ")\n",
    "val_loader = DataLoader(dataset=test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in_memory = []\n",
    "train_size = len(train_dataset)\n",
    "print(train_size)\n",
    "for i in tqdm(range(train_size)):\n",
    "    train_in_memory.append((train_dataset[i][0],train_dataset[i][1]))\n",
    "\n",
    "val_in_memory = []\n",
    "val_size = len(test_dataset)\n",
    "print(val_size)\n",
    "for i in tqdm(range(val_size)):\n",
    "    val_in_memory.append((test_dataset[i][0],test_dataset[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transToPIL = transforms.ToPILImage()\n",
    "count = 0\n",
    "for x,y in train_in_memory:\n",
    "    for xx,yy in zip(x,y):\n",
    "        if yy==401:\n",
    "            display(transToPIL(xx))\n",
    "    count+=1\n",
    "    if count == 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "\n",
    "    acc = acc * 100\n",
    "    \n",
    "    return acc\n",
    "\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin training.\")\n",
    "EPOCHS = 5\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    model_class.train()\n",
    "    count = 0\n",
    "    for X_train_batch, y_train_batch in train_in_memory:\n",
    "        count+=1\n",
    "        if count%1000==0: print(count)\n",
    "        #X_train_batch = X_train_batch.squeeze(0)\n",
    "        #y_train_batch = y_train_batch.squeeze(0)\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_train_pred = model(X_train_batch)[1]\n",
    "        y_train_pred = y_train_pred.squeeze(-1)\n",
    "        y_train_pred = y_train_pred.squeeze(-1)\n",
    "        y_train_pred = model_class(y_train_pred)\n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        model_class.eval()\n",
    "        for X_val_batch, y_val_batch in val_in_memory:\n",
    "            X_val_batch = X_val_batch.squeeze(0)\n",
    "            y_val_batch = y_val_batch.squeeze(0)\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "        \n",
    "            y_val_pred = model(X_val_batch)[1]\n",
    "            y_val_pred = y_val_pred.squeeze(-1)\n",
    "            y_val_pred = y_val_pred.squeeze(-1)\n",
    "            y_val_pred = model_class(y_val_pred)       \n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_val_batch, y_val_batch in val_in_memory:\n",
    "        X_val_batch = X_val_batch.to(device)\n",
    "        y_val_pred = model(X_val_batch)[1]\n",
    "        y_val_pred = y_val_pred.squeeze(-1)\n",
    "        y_val_pred = y_val_pred.squeeze(-1)\n",
    "        y_val_pred = model_class(y_val_pred)\n",
    "        y_pred_softmax = torch.log_softmax(y_val_pred, dim = 1)\n",
    "        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "        x = y_pred_tags.cpu().numpy()\n",
    "        y_pred_list += list(x)\n",
    "        y_test_list += list(y_val_batch.numpy())\n",
    "#y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model_class.state_dict(), \"vgg2_classifier_500.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([1 for x,y in zip(y_pred_list,y_test_list) if x==y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_list[2])\n",
    "print(y_test_list[2])\n",
    "print(classification_report(y_test_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('renv': venv)",
   "language": "python",
   "name": "python36964bitrenvvenv63ce17223ffb457ca2f50b70abb95e80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
